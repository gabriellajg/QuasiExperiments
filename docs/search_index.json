[["index.html", "R Book for Quasi-Experimental Designs Chapter 1 Course", " R Book for Quasi-Experimental Designs Ge Jiang QUERIES, University of Illinois at Urbana-Champaign Chapter 1 Course Welcome to Quasi-Experimental Designs! This course focuses on the analysis of some of the strongest quasi-experimental designs such as regression discontinuity, interrupted time series, propensity score matching, and instrumental variable methods. Social scientists have become increasingly interested in the causal effects of specific policies or practices. For example, an education researcher may want to know if curriculum A produces higher reading achievement scores than curriculum B does for third-graders. And if there is a difference in achievement scores between the two groups of students, what is the magnitude of the effect? Experiments (e.g., randomized control trials) are often referred to as the “gold standard” for determining the effect of a policy or practice (relative to some other policy or practice) on a population of interest. However, experimental designs can in some cases be time-consuming, costly, unethical, or otherwise impractical. Given these considerations and the wealth of already existing observational data, researchers have crafted careful approximations to randomized control trials that utilize the already existing data to learn more about social science phenomena. This class of research designs is referred to as “quasi-experimental designs.” Causal research questions like “Did No Child Left Behind (NCLB) increase students’ achievement in reading and math?” or “Does retaining kindergartners for one year (instead of promoting them) result in negative effects on their future achievements?” are typically investigated using quasi-experimental designs. Specifically, we will learn about the assumptions, theories, and application of each of the prominent quasi-experimental methods. This site is supposed to serve as a repository for R codes used in lab sessions of a graduate-level method course EPSY 574. *Disclaimer: Opinions are my own and not the views of my employer. "],["regression-approach-to-treatment-effect-estimation.html", "Chapter 2 Regression Approach to Treatment Effect Estimation 2.1 Regression w/ no confounder 2.2 Including confounder SES", " Chapter 2 Regression Approach to Treatment Effect Estimation Suppose one would like to use a regression model to estimate the treatment effect of a SAT, but controlling for the covariate ‘SES’. Simulating data: ID = c(1:6) Grp = rep(c(0, 1), each = 3) Score = c(550, 600, 650, 600, 720, 630) SES = c(1, 2, 2, 2, 3, 2) SATdat = data.frame(ID, Grp, Score, SES) SATdat$SES = factor(SATdat$SES) 2.1 Regression w/ no confounder lm1 = lm(Score ~ Grp, data = SATdat) summary(lm1) ## ## Call: ## lm(formula = Score ~ Grp, data = SATdat) ## ## Residuals: ## 1 2 3 4 5 6 ## -5.000e+01 -7.638e-14 5.000e+01 -5.000e+01 7.000e+01 -2.000e+01 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 600.00 32.66 18.371 5.17e-05 *** ## Grp 50.00 46.19 1.083 0.34 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56.57 on 4 degrees of freedom ## Multiple R-squared: 0.2266, Adjusted R-squared: 0.03323 ## F-statistic: 1.172 on 1 and 4 DF, p-value: 0.3399 This is the same as a simple t-test: t.test(SATdat$Score[SATdat$Grp==1], SATdat$Score[SATdat$Grp==0]) ## ## Welch Two Sample t-test ## ## data: SATdat$Score[SATdat$Grp == 1] and SATdat$Score[SATdat$Grp == 0] ## t = 1.0825, df = 3.8173, p-value = 0.3426 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -80.69522 180.69522 ## sample estimates: ## mean of x mean of y ## 650 600 not sig – sample size is too small 2.2 Including confounder SES ses.lm = lm(Score ~ Grp+SES, data = SATdat) summary(ses.lm) ## ## Call: ## lm(formula = Score ~ Grp + SES, data = SATdat) ## ## Residuals: ## 1 2 3 4 5 6 ## 1.066e-14 -2.500e+01 2.500e+01 -1.500e+01 -4.016e-15 1.500e+01 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 550.00 29.15 18.865 0.0028 ** ## Grp -10.00 29.15 -0.343 0.7643 ## SES2 75.00 35.71 2.100 0.1705 ## SES3 180.00 50.50 3.565 0.0705 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.15 on 2 degrees of freedom ## Multiple R-squared: 0.8973, Adjusted R-squared: 0.7432 ## F-statistic: 5.824 on 3 and 2 DF, p-value: 0.1501 The effect (-10) is assumed between groups WITHIN EACH SES LEVEL: 2.2.1 low SES predict(ses.lm, newdata = data.frame(Grp=0, SES=factor(1))) ## 1 ## 550 predict(ses.lm, newdata = data.frame(Grp=1, SES=factor(1))) ## 1 ## 540 2.2.2 middle SES predict(ses.lm, newdata = data.frame(Grp=0, SES=factor(2))) ## 1 ## 625 predict(ses.lm, newdata = data.frame(Grp=1, SES=factor(2))) ## 1 ## 615 2.2.3 high SES predict(ses.lm, newdata = data.frame(Grp=0, SES=factor(3))) ## 1 ## 730 predict(ses.lm, newdata = data.frame(Grp=1, SES=factor(3))) ## 1 ## 720 "],["types-of-causal-effects.html", "Chapter 3 Types of Causal Effects 3.1 ATT, ATE, ATU, … what the what? 3.2 Calculating different causal effects in practice", " Chapter 3 Types of Causal Effects Sometimes our experiments because quasi-experiments because of crossover or non-compliance. Simulating data on assignment, compliance, and potential outcomes: set.seed(1) assign = rbinom(100, 1, 0.5) # half half Simulating non-compliance as a function of gender: set.seed(101) gender = rbinom(100, 1, 0.4) # 40% are men comply = rep(); comply[gender==1] = rbinom(sum(gender==1), 1, 0.5) # prob of men complying is 0.5 comply[gender==0] = rbinom(sum(gender==0), 1, 0.7) # prob of women complying is 0.7 treatment effect: 100 (from 400 –&gt; 500) fixed for everyone: notutor = rnorm(100, 400, 12) # Y0 tutor = notutor + 100 + rnorm(100, 0, 20) # Y1 simdata = data.frame(assign, notutor, tutor) head(simdata) ## assign notutor tutor ## 1 0 403.2168 499.9361 ## 2 0 392.8935 465.2280 ## 3 1 425.6018 534.0721 ## 4 1 414.0730 498.2632 ## 5 0 408.9611 533.1596 ## 6 1 397.2339 515.1242 actual treatment: treat = ifelse(comply==1, assign, 1-assign) table(assign, treat) ## treat ## assign 0 1 ## 0 31 21 ## 1 22 26 observed value: obs = treat*tutor + (1-treat)*notutor # Y1 if T=1 and Y0 if T=0 when treat=1: Y = 1tutor + 0notutor = Y1 when treat=0: Y = 0tutor + 1notutor = Y0 Simulated data: simdata = data.frame(assign, comply, treat, tutor, notutor, obs, gender) head(simdata) ## assign comply treat tutor notutor obs gender ## 1 0 1 0 499.9361 403.2168 403.2168 0 ## 2 0 1 0 465.2280 392.8935 392.8935 0 ## 3 1 0 0 534.0721 425.6018 425.6018 1 ## 4 1 0 0 498.2632 414.0730 414.0730 1 ## 5 0 1 0 533.1596 408.9611 408.9611 0 ## 6 1 1 1 515.1242 397.2339 515.1242 0 3.1 ATT, ATE, ATU, … what the what? ATE/ACE: Average Treatment Effect in the population ATT: Average Treatment Effect on the Treated ATU: Average Treatment Effect on the Untreated LATE: Local Average Treatment Effect for Compilers CATE: Conditional Average Treatment Effect ITT: Intention To Treat. Effect of simply the intention (instead of receipt) of treatment ATE = mean(simdata$tutor - simdata$notutor) # whole population ATT = mean((simdata$tutor - simdata$notutor)[treat==1]) ATU = mean((simdata$tutor - simdata$notutor)[treat==0]) LATE = mean((simdata$tutor - simdata$notutor)[comply==1]) CATE0 = mean((simdata$tutor - simdata$notutor)[gender==0]) CATE1 = mean((simdata$tutor - simdata$notutor)[gender==1]) cbind(ATE, ATT, ATU, LATE, CATE0, CATE1) ## ATE ATT ATU LATE CATE0 CATE1 ## [1,] 100.0414 101.1523 99.05629 99.88937 102.4829 97.28824 # all close to ATE 3.2 Calculating different causal effects in practice ATE ATE_obs = lm(obs ~ treat, data = simdata) coef(ATE_obs) # 99.82048 ## (Intercept) treat ## 400.12192 99.82048 ATT &amp; ATU no way to estimate ATT &amp; ATU from data LATE if compliance status is known LATE_obs = lm(obs ~ treat, data = simdata[simdata$comply==1,]) coef(LATE_obs) # 101.1594 ## (Intercept) treat ## 398.5844 101.1594 CATE if gender is included as a confounder: CATE0_obs = lm(obs ~ treat, data = simdata[simdata$gender==0,]) coef(CATE0_obs) # 106.5414 ## (Intercept) treat ## 400.1959 106.5414 CATE1_obs = lm(obs ~ treat, data = simdata[simdata$gender==1,]) coef(CATE1_obs) # 90.71861 ## (Intercept) treat ## 400.05067 90.71861 Intention To Treat (ITT) ITT = lm(obs ~ assign) coef(ITT) # 15.460 whereas the true effect is 100!! ## (Intercept) assign ## 439.61675 15.45999 Intention To Treat on Compliers ITT_c = lm(obs ~ assign, data = simdata[simdata$comply==1,]) coef(ITT_c) # 101.159 which is pretty close to the true effect 100 ## (Intercept) assign ## 398.5844 101.1594 Generally recommended: LATE or ITT on Compliers! "],["instrumental-variable.html", "Chapter 4 Instrumental Variable 4.1 Loading Data 4.2 Setup 4.3 Regression Approaches 4.4 IV: Does education effect wages when college proximity is used as the instrument? 4.5 Take-home exercise", " Chapter 4 Instrumental Variable In this lab, we will use Card (1995) to see how we can use an instrumental variable (distance to the nearest college) to estimate the effect of schooling on wage. 4.1 Loading Data Load Stata file: library(psych) library(foreign) library(car) card.data&lt;-read.dta(&quot;card.dta&quot;) See the description of the data here: https://www.ssc.wisc.edu/~bhansen/econometrics/Card1995_description.pdf Preview the data: attach(card.data) ## The following objects are masked from bwght (pos = 3): ## ## fatheduc, motheduc ## The following objects are masked from card.data (pos = 4): ## ## age, black, educ, enroll, exper, expersq, fatheduc, id, iq, kww, ## libcrd14, lwage, married, momdad14, motheduc, nearc2, nearc4, reg661, ## reg662, reg663, reg664, reg665, reg666, reg667, reg668, reg669, sinmom14, ## smsa, smsa66, south, south66, step14, wage, weight ## The following objects are masked from bwght (pos = 56): ## ## fatheduc, motheduc ## The following objects are masked from card.data (pos = 63): ## ## age, black, educ, enroll, exper, expersq, fatheduc, id, iq, kww, ## libcrd14, lwage, married, momdad14, motheduc, nearc2, nearc4, reg661, ## reg662, reg663, reg664, reg665, reg666, reg667, reg668, reg669, sinmom14, ## smsa, smsa66, south, south66, step14, wage, weight head(card.data) ## id nearc2 nearc4 educ age fatheduc motheduc weight momdad14 sinmom14 step14 reg661 ## 1 2 0 0 7 29 NA NA 158413 1 0 0 1 ## 2 3 0 0 12 27 8 8 380166 1 0 0 1 ## 3 4 0 0 12 34 14 12 367470 1 0 0 1 ## 4 5 1 1 11 27 11 12 380166 1 0 0 0 ## 5 6 1 1 12 34 8 7 367470 1 0 0 0 ## 6 7 1 1 12 26 9 12 380166 1 0 0 0 ## reg662 reg663 reg664 reg665 reg666 reg667 reg668 reg669 south66 black smsa south ## 1 0 0 0 0 0 0 0 0 0 1 1 0 ## 2 0 0 0 0 0 0 0 0 0 0 1 0 ## 3 0 0 0 0 0 0 0 0 0 0 1 0 ## 4 1 0 0 0 0 0 0 0 0 0 1 0 ## 5 1 0 0 0 0 0 0 0 0 0 1 0 ## 6 1 0 0 0 0 0 0 0 0 0 1 0 ## smsa66 wage enroll kww iq married libcrd14 exper lwage expersq ## 1 1 548 0 15 NA 1 0 16 6.306275 256 ## 2 1 481 0 35 93 1 1 9 6.175867 81 ## 3 1 721 0 42 103 1 1 16 6.580639 256 ## 4 1 250 0 25 88 1 1 10 5.521461 100 ## 5 1 729 0 34 108 1 0 16 6.591674 256 ## 6 1 500 0 38 85 1 1 8 6.214608 64 4.2 Setup Treatment: educ (years of education) Outcome: lwage (log_wage) IV: nearc4 (whether live close to a four-year college or not) pairs.panels(card.data[,c(&#39;lwage&#39;,&#39;educ&#39;, &#39;nearc4&#39;)]) 4.3 Regression Approaches 4.3.1 Naive Regression: OLS estimate with treatment only m0&lt;-lm(lwage~educ, data = card.data) summary(m0) ## ## Call: ## lm(formula = lwage ~ educ, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.73799 -0.27764 0.02373 0.28839 1.46080 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.57088 0.03883 143.47 &lt;2e-16 *** ## educ 0.05209 0.00287 18.15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4214 on 3008 degrees of freedom ## Multiple R-squared: 0.09874, Adjusted R-squared: 0.09844 ## F-statistic: 329.5 on 1 and 3008 DF, p-value: &lt; 2.2e-16 4.3.2 Regression with covariates We find education is SSD, but we can make the case that it is endogenous. m1&lt;-lm(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66, data = card.data) summary(m1) ## ## Call: ## lm(formula = lwage ~ educ + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.62326 -0.22141 0.02001 0.23932 1.33340 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.7393766 0.0715282 66.259 &lt; 2e-16 *** ## educ 0.0746933 0.0034983 21.351 &lt; 2e-16 *** ## exper 0.0848320 0.0066242 12.806 &lt; 2e-16 *** ## expersq -0.0022870 0.0003166 -7.223 6.41e-13 *** ## black -0.1990123 0.0182483 -10.906 &lt; 2e-16 *** ## south -0.1479550 0.0259799 -5.695 1.35e-08 *** ## smsa 0.1363845 0.0201005 6.785 1.39e-11 *** ## reg661 -0.1185698 0.0388301 -3.054 0.002281 ** ## reg662 -0.0222026 0.0282575 -0.786 0.432092 ## reg663 0.0259703 0.0273644 0.949 0.342670 ## reg664 -0.0634942 0.0356803 -1.780 0.075254 . ## reg665 0.0094551 0.0361174 0.262 0.793503 ## reg666 0.0219476 0.0400984 0.547 0.584182 ## reg667 -0.0005887 0.0393793 -0.015 0.988073 ## reg668 -0.1750058 0.0463394 -3.777 0.000162 *** ## smsa66 0.0262417 0.0194477 1.349 0.177327 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3723 on 2994 degrees of freedom ## Multiple R-squared: 0.2998, Adjusted R-squared: 0.2963 ## F-statistic: 85.48 on 15 and 2994 DF, p-value: &lt; 2.2e-16 4.4 IV: Does education effect wages when college proximity is used as the instrument? Is college proximity an exogenous determinant of wages? 4.4.1 Stage 1: T on IV m2&lt;-lm(educ~nearc4+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66, data = card.data) summary(m2) ## ## Call: ## lm(formula = educ ~ nearc4 + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.545 -1.370 -0.091 1.278 6.239 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.8485239 0.2111222 79.805 &lt; 2e-16 *** ## nearc4 0.3198989 0.0878638 3.641 0.000276 *** ## exper -0.4125334 0.0336996 -12.241 &lt; 2e-16 *** ## expersq 0.0008686 0.0016504 0.526 0.598728 ## black -0.9355287 0.0937348 -9.981 &lt; 2e-16 *** ## south -0.0516126 0.1354284 -0.381 0.703152 ## smsa 0.4021825 0.1048112 3.837 0.000127 *** ## reg661 -0.2102710 0.2024568 -1.039 0.299076 ## reg662 -0.2889073 0.1473395 -1.961 0.049992 * ## reg663 -0.2382099 0.1426357 -1.670 0.095012 . ## reg664 -0.0930890 0.1859827 -0.501 0.616742 ## reg665 -0.4828875 0.1881872 -2.566 0.010336 * ## reg666 -0.5130857 0.2096352 -2.448 0.014442 * ## reg667 -0.4270887 0.2056208 -2.077 0.037880 * ## reg668 0.3136204 0.2416739 1.298 0.194490 ## smsa66 0.0254805 0.1057692 0.241 0.809644 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.941 on 2994 degrees of freedom ## Multiple R-squared: 0.4771, Adjusted R-squared: 0.4745 ## F-statistic: 182.1 on 15 and 2994 DF, p-value: &lt; 2.2e-16 Predicted part in T: educ_hat = fitted(m2) 4.4.1.1 test of weak instrument - rejected linearHypothesis(m2, c(&quot;nearc4=0&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## nearc4 = 0 ## ## Model 1: restricted model ## Model 2: educ ~ nearc4 + exper + expersq + black + south + smsa + reg661 + ## reg662 + reg663 + reg664 + reg665 + reg666 + reg667 + reg668 + ## smsa66 ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 2995 11324 ## 2 2994 11274 1 49.917 13.256 0.0002763 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.4.2 Stage 2: Y on T_hat m2b = lm(lwage~ educ_hat+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66, data = card.data) summary(m2b) ## ## Call: ## lm(formula = lwage ~ educ_hat + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.57387 -0.25161 0.01483 0.27229 1.38522 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7739651 0.9612564 3.926 8.83e-05 *** ## educ_hat 0.1315038 0.0565103 2.327 0.020028 * ## exper 0.1082711 0.0243243 4.451 8.85e-06 *** ## expersq -0.0023349 0.0003429 -6.810 1.18e-11 *** ## black -0.1467757 0.0554166 -2.649 0.008125 ** ## south -0.1446715 0.0280524 -5.157 2.67e-07 *** ## smsa 0.1118083 0.0325530 3.435 0.000601 *** ## reg661 -0.1078142 0.0429903 -2.508 0.012199 * ## reg662 -0.0070465 0.0338333 -0.208 0.835032 ## reg663 0.0404445 0.0326749 1.238 0.215892 ## reg664 -0.0579172 0.0386641 -1.498 0.134250 ## reg665 0.0384577 0.0482596 0.797 0.425577 ## reg666 0.0550887 0.0541416 1.017 0.309001 ## reg667 0.0267580 0.0502027 0.533 0.594074 ## reg668 -0.1908912 0.0521383 -3.661 0.000255 *** ## smsa66 0.0185311 0.0222167 0.834 0.404286 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3993 on 2994 degrees of freedom ## Multiple R-squared: 0.1947, Adjusted R-squared: 0.1907 ## F-statistic: 48.25 on 15 and 2994 DF, p-value: &lt; 2.2e-16 4.4.3 All-in-one function ivreg(): library(AER) m4&lt;-ivreg(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66 | nearc4+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66) formula, instruments formula specification(s) of the regression relationship and the instruments. instruments is missing and formula has three parts as in y ~ x1 + x2 | z1 + z2 + z3 (recommended) Alternatively: m4&lt;-ivreg(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66 | .-educ+nearc4) to the left of |: outcome = lwage to the left of |: variables = educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+reg664+reg665+reg666+reg667+reg668+smsa66 to the right of |: variables = nearc4+exper+expersq+black+south+smsa+reg661+reg662+reg663+reg664+reg665+reg666+reg667+reg668+smsa66 to the right of |: outcome = educ to the right of |: 1st stage of 2SLS to the left of |: 2nd stage of 2SLS ‘.’ means all variables in the dataset besides the one to the left of lwage treatment variable instrument Test using sandwich standard errors: summary(m4, vcov = sandwich, diagnostics = TRUE) ## ## Call: ## ivreg(formula = lwage ~ educ + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66 | . - educ + nearc4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.83164 -0.24075 0.02428 0.25208 1.42760 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7739651 0.9174053 4.114 4.00e-05 *** ## educ 0.1315038 0.0539995 2.435 0.014938 * ## exper 0.1082711 0.0233466 4.638 3.68e-06 *** ## expersq -0.0023349 0.0003478 -6.713 2.27e-11 *** ## black -0.1467757 0.0523622 -2.803 0.005094 ** ## south -0.1446715 0.0290653 -4.977 6.81e-07 *** ## smsa 0.1118083 0.0310619 3.600 0.000324 *** ## reg661 -0.1078142 0.0409668 -2.632 0.008538 ** ## reg662 -0.0070465 0.0336994 -0.209 0.834387 ## reg663 0.0404445 0.0325208 1.244 0.213725 ## reg664 -0.0579172 0.0392106 -1.477 0.139759 ## reg665 0.0384577 0.0494675 0.777 0.436965 ## reg666 0.0550887 0.0521309 1.057 0.290716 ## reg667 0.0267580 0.0501066 0.534 0.593367 ## reg668 -0.1908912 0.0506897 -3.766 0.000169 *** ## smsa66 0.0185311 0.0205103 0.904 0.366333 ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 1 2994 14.214 0.000166 *** ## Wu-Hausman 1 2993 1.219 0.269649 ## Sargan 0 NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3883 on 2994 degrees of freedom ## Multiple R-Squared: 0.2382, Adjusted R-squared: 0.2343 ## Wald test: 56.06 on 15 and 2994 DF, p-value: &lt; 2.2e-16 4.4.4 Diagnostic tests: df1 df2 statistic p-value Weak instruments 1 2994 14.214 0.000166 *** Wu-Hausman 1 2993 1.219 0.269649 Sargan 0 NA NA NA Weak instruments means that the IV has a low correlation with the treatment variable. The null is that the IV is weak. If the null is rejected, so you can move forward with the assumption that the instrument is sufficiently strong. Applied to 2SLS regression, the Wu–Hausman test is a test of endogenity. If all of the regressors are exogenous, then both the OLS and 2SLS estimators are consistent, and the OLS estimator is more efficient, but if one or more regressors are endogenous, then the OLS estimator is inconsistent. A large test statistic and small p-value suggests that the OLS estimator is inconsistent and the 2SLS estimator is therefore to be preferred. 4.4.5 Multiple IV #m5&lt;-ivreg(lwage~educ | .-educ+nearc4+nearc2) m5&lt;-ivreg(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+reg664+reg665+reg666+reg667+reg668+smsa66 | .-educ+nearc4+nearc2) summary(m5, vcov = sandwich, diagnostics = TRUE) ## ## Call: ## ivreg(formula = lwage ~ educ + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66 | . - educ + nearc4 + nearc2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.93841 -0.25068 0.01932 0.26519 1.46998 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.3396868 0.8909170 3.749 0.000181 *** ## educ 0.1570594 0.0524127 2.997 0.002753 ** ## exper 0.1188149 0.0228905 5.191 2.24e-07 *** ## expersq -0.0023565 0.0003674 -6.414 1.64e-10 *** ## black -0.1232778 0.0514904 -2.394 0.016718 * ## south -0.1431945 0.0301873 -4.744 2.20e-06 *** ## smsa 0.1007530 0.0313621 3.213 0.001329 ** ## reg661 -0.1029760 0.0425755 -2.419 0.015637 * ## reg662 -0.0002286 0.0345230 -0.007 0.994716 ## reg663 0.0469556 0.0335252 1.401 0.161435 ## reg664 -0.0554084 0.0408927 -1.355 0.175529 ## reg665 0.0515041 0.0506274 1.017 0.309085 ## reg666 0.0699968 0.0534531 1.309 0.190466 ## reg667 0.0390596 0.0514309 0.759 0.447639 ## reg668 -0.1980371 0.0522335 -3.791 0.000153 *** ## smsa66 0.0150626 0.0211120 0.713 0.475616 ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 2 2993 8.366 0.000238 *** ## Wu-Hausman 1 2993 2.978 0.084509 . ## Sargan 1 NA 1.248 0.263905 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4053 on 2994 degrees of freedom ## Multiple R-Squared: 0.1702, Adjusted R-squared: 0.166 ## Wald test: 51.65 on 15 and 2994 DF, p-value: &lt; 2.2e-16 Sargan tests overidentification restrictions. The idea is that if you have more than one instrument per endogenous variable, the model is overidentified, and you have some excess information. All of the instruments must be valid for the inferences to be correct. So it tests that all exogenous instruments are in fact exogenous, and uncorrelated with the model residuals. If it is significant, it means that you don’t have valid instruments (somewhere in there, as this is a global test). If it is not significant (our case), this isn’t a concern. 4.4.6 Compare with OLS: library(stargazer) stargazer::stargazer(m0, m1, m2b, m4, type = &#39;text&#39;, model.names = FALSE, column.labels = c(&#39;Naive&#39;, &#39;OLS&#39;, &#39;2SLS&#39;, &#39;IV&#39;), column.sep.width = &quot;15pt&quot;, omit.stat = c(&quot;f&quot;, &quot;ser&quot;)) ## ## =================================================== ## Dependent variable: ## -------------------------------------- ## lwage ## Naive OLS 2SLS IV ## (1) (2) (3) (4) ## --------------------------------------------------- ## educ 0.052*** 0.075*** 0.132** ## (0.003) (0.003) (0.055) ## ## educ_hat 0.132** ## (0.057) ## ## exper 0.085*** 0.108*** 0.108*** ## (0.007) (0.024) (0.024) ## ## expersq -0.002*** -0.002*** -0.002*** ## (0.0003) (0.0003) (0.0003) ## ## black -0.199*** -0.147*** -0.147*** ## (0.018) (0.055) (0.054) ## ## south -0.148*** -0.145*** -0.145*** ## (0.026) (0.028) (0.027) ## ## smsa 0.136*** 0.112*** 0.112*** ## (0.020) (0.033) (0.032) ## ## reg661 -0.119*** -0.108** -0.108*** ## (0.039) (0.043) (0.042) ## ## reg662 -0.022 -0.007 -0.007 ## (0.028) (0.034) (0.033) ## ## reg663 0.026 0.040 0.040 ## (0.027) (0.033) (0.032) ## ## reg664 -0.063* -0.058 -0.058 ## (0.036) (0.039) (0.038) ## ## reg665 0.009 0.038 0.038 ## (0.036) (0.048) (0.047) ## ## reg666 0.022 0.055 0.055 ## (0.040) (0.054) (0.053) ## ## reg667 -0.001 0.027 0.027 ## (0.039) (0.050) (0.049) ## ## reg668 -0.175*** -0.191*** -0.191*** ## (0.046) (0.052) (0.051) ## ## smsa66 0.026 0.019 0.019 ## (0.019) (0.022) (0.022) ## ## Constant 5.571*** 4.739*** 3.774*** 3.774*** ## (0.039) (0.072) (0.961) (0.935) ## ## --------------------------------------------------- ## Observations 3,010 3,010 3,010 3,010 ## R2 0.099 0.300 0.195 0.238 ## Adjusted R2 0.098 0.296 0.191 0.234 ## =================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 4.5 Take-home exercise 4.5.1 Does cigarette smoking have an effect on child birth weight (Wooldridge, 2002)? bwght&lt;-read.dta(&quot;bwght.dta&quot;) head(bwght) ## faminc cigtax cigprice bwght fatheduc motheduc parity male white cigs lbwght ## 1 13.5 16.5 122.3 109 12 12 1 1 1 0 4.691348 ## 2 7.5 16.5 122.3 133 6 12 2 1 0 0 4.890349 ## 3 0.5 16.5 122.3 129 NA 12 2 0 0 0 4.859812 ## 4 15.5 16.5 122.3 126 12 12 2 1 0 0 4.836282 ## 5 27.5 16.5 122.3 134 14 12 2 1 1 0 4.897840 ## 6 7.5 16.5 122.3 118 12 14 6 1 0 0 4.770685 ## bwghtlbs packs lfaminc ## 1 6.8125 0 2.6026897 ## 2 8.3125 0 2.0149031 ## 3 8.0625 0 -0.6931472 ## 4 7.8750 0 2.7408400 ## 5 8.3750 0 3.3141861 ## 6 7.3750 0 2.0149031 attach(bwght) 4.5.2 Missing data colSums(apply(bwght, 2, is.na)) ## faminc cigtax cigprice bwght fatheduc motheduc parity male white ## 0 0 0 0 196 1 0 0 0 ## cigs lbwght bwghtlbs packs lfaminc ## 0 0 0 0 0 https://rdrr.io/cran/wooldridge/man/bwght.html A data.frame with 1388 observations on 14 variables: cigtax: cig. tax in home state, 1988 cigprice: cig. price in home state, 1988 (tax already included) faminc: 1988 family income, $1000s lfaminc: log(faminc) fatheduc: father’s yrs of educ motheduc: mother’s yrs of educ parity: birth order of child male: =1 if male child white: =1 if white cigs: cigs smked per day while preg packs: packs smked per day while preg lbwght: log of bwght bwghtlbs: birth weight, pounds bwght: birth weight, ounces library(psych) pairs.panels(bwght[,c(&#39;faminc&#39;,&#39;lfaminc&#39;)]) # lfaminc is better pairs.panels(bwght[,c(&#39;cigs&#39;,&#39;packs&#39;)]) # same, go with cigs pairs.panels(bwght[,c(&#39;lbwght&#39;,&#39;bwghtlbs&#39;, &#39;bwght&#39;)]) # any is good, go with bwghtlbs pairs.panels(bwght[,c(&#39;cigs&#39;,&#39;bwghtlbs&#39;, &#39;cigprice&#39;)]) # low correlations pairs.panels(bwght[,c(&#39;cigs&#39;,&#39;bwghtlbs&#39;, &#39;cigprice&#39;, &#39;lfaminc&#39;, &#39;fatheduc&#39;, &#39;motheduc&#39;, &#39;parity&#39;, &#39;male&#39;, &#39;white&#39;)]) 4.5.3 Your turn now!!!! 4.5.4 Naive Regression: OLS Estimate with treatment only m0&lt;-lm(bwghtlbs ~ cigs, data = bwght) summary(m0) ## ## Call: ## lm(formula = bwghtlbs ~ cigs, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0482 -0.7357 0.0186 0.8268 9.4518 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.485744 0.035771 209.267 &lt; 2e-16 *** ## cigs -0.032111 0.005656 -5.678 1.66e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.258 on 1386 degrees of freedom ## Multiple R-squared: 0.02273, Adjusted R-squared: 0.02202 ## F-statistic: 32.24 on 1 and 1386 DF, p-value: 1.662e-08 4.5.5 Regression with covariates m1&lt;-lm(bwghtlbs ~ cigs+lfaminc+fatheduc+motheduc+parity+male+white, data = bwght) summary(m1) ## ## Call: ## lm(formula = bwghtlbs ~ cigs + lfaminc + fatheduc + motheduc + ## parity + male + white, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8763 -0.7293 0.0241 0.7912 9.5145 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.658642 0.254769 26.136 &lt; 2e-16 *** ## cigs -0.037336 0.006856 -5.446 6.27e-08 *** ## lfaminc 0.076288 0.057027 1.338 0.181234 ## fatheduc 0.025947 0.017417 1.490 0.136565 ## motheduc -0.021047 0.019852 -1.060 0.289270 ## parity 0.119845 0.040955 2.926 0.003496 ** ## male 0.239041 0.071462 3.345 0.000849 *** ## white 0.289841 0.100741 2.877 0.004085 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.228 on 1183 degrees of freedom ## (197 observations deleted due to missingness) ## Multiple R-squared: 0.05444, Adjusted R-squared: 0.04885 ## F-statistic: 9.731 on 7 and 1183 DF, p-value: 7.985e-12 4.5.5.1 test of weak instrument - rejected m2&lt;-lm(cigs ~ cigprice+lfaminc+fatheduc+motheduc+parity+male+white, data = bwght) summary(m2) ## ## Call: ## lm(formula = cigs ~ cigprice + lfaminc + fatheduc + motheduc + ## parity + male + white, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.509 -2.309 -1.466 -0.148 37.113 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.13393 2.09009 3.413 0.000664 *** ## cigprice 0.01366 0.01473 0.928 0.353749 ## lfaminc -0.55819 0.24177 -2.309 0.021129 * ## fatheduc -0.11362 0.07385 -1.538 0.124201 ## motheduc -0.33481 0.08360 -4.005 6.58e-05 *** ## parity 0.13511 0.17357 0.778 0.436485 ## male -0.37175 0.30303 -1.227 0.220151 ## white 0.64796 0.42747 1.516 0.129839 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.204 on 1183 degrees of freedom ## (197 observations deleted due to missingness) ## Multiple R-squared: 0.05705, Adjusted R-squared: 0.05147 ## F-statistic: 10.22 on 7 and 1183 DF, p-value: 1.753e-12 library(car) linearHypothesis(m2, c(&quot;cigprice=0&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## cigprice = 0 ## ## Model 1: restricted model ## Model 2: cigs ~ cigprice + lfaminc + fatheduc + motheduc + parity + male + ## white ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 1184 32066 ## 2 1183 32043 1 23.311 0.8606 0.3537 cigprice, tax included, penalizes the excessive use of cigarettes although cigprice affects cigs, cigprice is not a strong instrument 4.5.6 IV done manually bwght.comp = na.omit(bwght) bwght.comp$cig_hat = fitted(m2) m3 = lm(bwghtlbs ~ cig_hat+lfaminc+fatheduc+motheduc+parity+male+white, data = bwght.comp) summary(m3) ## ## Call: ## lm(formula = bwghtlbs ~ cig_hat + lfaminc + fatheduc + motheduc + ## parity + male + white, data = bwght.comp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8116 -0.7267 0.0585 0.8156 9.5607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.67348 2.28159 2.048 0.04075 * ## cig_hat 0.18797 0.25739 0.730 0.46534 ## lfaminc 0.19868 0.15122 1.314 0.18915 ## fatheduc 0.05079 0.03340 1.521 0.12863 ## motheduc 0.05459 0.08869 0.616 0.53828 ## parity 0.08942 0.05409 1.653 0.09855 . ## male 0.32012 0.11750 2.724 0.00654 ** ## white 0.13817 0.20099 0.687 0.49193 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.243 on 1183 degrees of freedom ## Multiple R-squared: 0.03118, Adjusted R-squared: 0.02544 ## F-statistic: 5.439 on 7 and 1183 DF, p-value: 3.691e-06 same coefficients with ivreg(), but incorrect standard errors 4.5.7 IVreg() library(AER) m4&lt;-ivreg(bwghtlbs ~ cigs+lfaminc+fatheduc+motheduc+parity+male+white | .-cigs+cigprice) summary(m4, vcov = sandwich, diagnostics = TRUE) ## ## Call: ## ivreg(formula = bwghtlbs ~ cigs + lfaminc + fatheduc + motheduc + ## parity + male + white | . - cigs + cigprice) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5996 -0.7003 0.2125 1.1268 9.8161 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.67348 3.03351 1.541 0.1237 ## cigs 0.18797 0.34402 0.546 0.5849 ## lfaminc 0.19868 0.19853 1.001 0.3172 ## fatheduc 0.05079 0.04496 1.130 0.2588 ## motheduc 0.05459 0.11806 0.462 0.6439 ## parity 0.08942 0.07452 1.200 0.2304 ## male 0.32012 0.15398 2.079 0.0378 * ## white 0.13817 0.27141 0.509 0.6108 ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 1 1183 0.914 0.339 ## Wu-Hausman 1 1182 0.825 0.364 ## Sargan 0 NA NA NA ## ## Residual standard error: 1.698 on 1183 degrees of freedom ## Multiple R-Squared: -0.8088, Adjusted R-squared: -0.8195 ## Wald test: 3 on 7 and 1183 DF, p-value: 0.003982 4.5.8 Model Comparison stargazer::stargazer(m0, m1, m3, m4, type = &#39;text&#39;, model.names = FALSE, column.labels = c(&#39;Naive&#39;, &#39;OLS&#39;, &#39;2SLS&#39;, &#39;IV&#39;), column.sep.width = &quot;15pt&quot;, omit.stat = c(&quot;f&quot;, &quot;ser&quot;)) ## ## ================================================= ## Dependent variable: ## ------------------------------------ ## bwghtlbs ## Naive OLS 2SLS IV ## (1) (2) (3) (4) ## ------------------------------------------------- ## cigs -0.032*** -0.037*** 0.188 ## (0.006) (0.007) (0.352) ## ## cig_hat 0.188 ## (0.257) ## ## lfaminc 0.076 0.199 0.199 ## (0.057) (0.151) (0.207) ## ## fatheduc 0.026 0.051 0.051 ## (0.017) (0.033) (0.046) ## ## motheduc -0.021 0.055 0.055 ## (0.020) (0.089) (0.121) ## ## parity 0.120*** 0.089* 0.089 ## (0.041) (0.054) (0.074) ## ## male 0.239*** 0.320*** 0.320** ## (0.071) (0.117) (0.161) ## ## white 0.290*** 0.138 0.138 ## (0.101) (0.201) (0.275) ## ## Constant 7.486*** 6.659*** 4.673** 4.673 ## (0.036) (0.255) (2.282) (3.118) ## ## ------------------------------------------------- ## Observations 1,388 1,191 1,191 1,191 ## R2 0.023 0.054 0.031 -0.809 ## Adjusted R2 0.022 0.049 0.025 -0.819 ## ================================================= ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Since cigprice is NOT a good instrument, we will favor m2 (OLS) over m4 in this case. Negative R2? https://www.stata.com/support/faqs/statistics/two-stage-least-squares/#:~:text=Stata’s%20ivregress%20command%20suppresses%20the,the%20context%20of%202SLS%2FIV. Missing R2s, negative R2s, and negative model sum of squares are all the same issue. However, since our goal is to estimate the structural model, the actual values, not the instruments for the endogenous right-hand-side variables, are used to determine the model sum of squares (MSS). This means a constant-only model of the dependent variable is NOT nested within the two-stage least-squares model, even though the two-stage model estimates an intercept, and the residual sum of squares (RSS) is no longer constrained to be smaller than the total sum of squares (TSS). ybar is a better predictor of y (in the sum-of-squares sense) than Xb! Is that a problem? You can easily develop simulations where the parameter estimates from two-stage are quite good while the MSS is negative. If our two-stage model produces estimates of these parameters with acceptable standard errors, we should be happy—regardless of MSS or R2. "],["psm-matching-strategy.html", "Chapter 5 PSM Matching Strategy 5.1 Data 5.2 Regression Estimates 5.3 PSM Steps", " Chapter 5 PSM Matching Strategy library(Matching) library(MatchIt) library(optmatch) library(weights) library(cem) library(tcltk2) library(knitr) library(CBPS) library(jtools) library(cobalt) library(lmtest) library(sandwich) #vcovCL library(rbounds) #gamma 5.1 Data #data(lalonde) lalonde = MatchIt::lalonde dim(lalonde) ## [1] 614 9 names(lalonde) ## [1] &quot;treat&quot; &quot;age&quot; &quot;educ&quot; &quot;race&quot; &quot;married&quot; &quot;nodegree&quot; &quot;re74&quot; ## [8] &quot;re75&quot; &quot;re78&quot; 5.1.1 Create dummy variable for race and unemployment lalonde$black = ifelse(lalonde$race==&#39;black&#39;, 1, 0) lalonde$hispan = ifelse(lalonde$race==&#39;hispan&#39;, 1, 0) lalonde$un74 = ifelse(lalonde$re74==0, 1, 0) lalonde$un75 = ifelse(lalonde$re75==0, 1, 0) 5.2 Regression Estimates 5.2.1 naive t-test estimate reglm &lt;- lm(re78 ~ treat, data = lalonde) summ(reglm) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(1,612) = 0.93, p = 0.33 ## R² = 0.00 ## Adj. R² = -0.00 ## ## Standard errors: OLS ## ---------------------------------------------------- ## Est. S.E. t val. p ## ----------------- --------- -------- -------- ------ ## (Intercept) 6984.17 360.71 19.36 0.00 ## treat -635.03 657.14 -0.97 0.33 ## ---------------------------------------------------- 5.2.2 regression with covariates reglm1 &lt;- lm(re78 ~ treat + educ + age + black + hispan + married + nodegree + un74 + un75 + re74 + re75, data = lalonde) summ(reglm1) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(11,602) = 10.75, p = 0.00 ## R² = 0.16 ## Adj. R² = 0.15 ## ## Standard errors: OLS ## ------------------------------------------------------ ## Est. S.E. t val. p ## ----------------- ---------- --------- -------- ------ ## (Intercept) -137.00 2422.92 -0.06 0.95 ## treat 865.59 800.01 1.08 0.28 ## educ 349.38 158.47 2.20 0.03 ## age -3.81 32.82 -0.12 0.91 ## black -1466.72 768.90 -1.91 0.06 ## hispan 471.11 934.51 0.50 0.61 ## married 445.25 690.83 0.64 0.52 ## nodegree 3.18 845.33 0.00 1.00 ## un74 2492.95 807.49 3.09 0.00 ## un75 315.32 758.16 0.42 0.68 ## re74 0.38 0.06 6.05 0.00 ## re75 0.31 0.12 2.69 0.01 ## ------------------------------------------------------ 5.3 PSM Steps 5.3.1 Selection of covariates in X fm1 = treat ~ age + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm2 = treat ~ age + I(age^2) + I(age^3) + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm3 = treat ~ age + I(age^2) + I(age^3) + educ + I(educ^2) + black + hispan + married + I(re74/1000) + I(re75/1000) 5.3.2 Calculation of propensity scores (p-scores) pscore &lt;- glm(fm2, data = lalonde, family = &#39;binomial&#39;) head(pscore$fitted.values) ## NSW1 NSW2 NSW3 NSW4 NSW5 NSW6 ## 0.5871794 0.3014102 0.9056376 0.9020139 0.9069395 0.8230760 hist(pscore$fitted.values[lalonde$treat==0],xlim=c(0,1)) hist(pscore$fitted.values[lalonde$treat==1],xlim=c(0,1)) lalonde$pscore = pscore$fitted.values Logistic regression: summ(pscore) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: treat ## Type: Generalized linear model ## Family: binomial ## Link function: logit ## ## MODEL FIT: ## χ²(9) = 310.83, p = 0.00 ## Pseudo-R² (Cragg-Uhler) = 0.56 ## Pseudo-R² (McFadden) = 0.41 ## AIC = 460.66, BIC = 504.86 ## ## Standard errors: MLE ## -------------------------------------------------- ## Est. S.E. z val. p ## ------------------ -------- ------ -------- ------ ## (Intercept) -18.92 4.22 -4.49 0.00 ## age 1.51 0.44 3.45 0.00 ## I(age^2) -0.04 0.01 -2.73 0.01 ## I(age^3) 0.00 0.00 2.01 0.04 ## educ -0.04 0.05 -0.76 0.45 ## black 3.06 0.30 10.17 0.00 ## hispan 0.68 0.45 1.51 0.13 ## married -1.34 0.31 -4.27 0.00 ## I(re74/1000) -0.10 0.03 -3.11 0.00 ## I(re75/1000) 0.04 0.05 0.82 0.41 ## -------------------------------------------------- try other formulas? 5.3.3 Matching based on p-sores all in one function matchit(): set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm1, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = TRUE, caliper = 0.2, discard = &#39;both&#39; ) summary of matching results: summary(m.out) ## ## Call: ## matchit(formula = fm1, data = lalonde, method = &quot;nearest&quot;, distance = &quot;logit&quot;, ## discard = &quot;both&quot;, replace = TRUE, caliper = 0.2) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.5722 0.1845 1.8020 0.8651 0.3763 0.6428 ## age 25.8162 28.0303 -0.3094 0.4400 0.0813 0.1577 ## educ 10.3459 10.2354 0.0550 0.4959 0.0347 0.1114 ## black 0.8432 0.2028 1.7615 . 0.6404 0.6404 ## hispan 0.0595 0.1422 -0.3498 . 0.0827 0.0827 ## married 0.1892 0.5128 -0.8263 . 0.3236 0.3236 ## I(re74/1000) 2.0956 5.6192 -0.7211 0.5181 0.2248 0.4470 ## I(re75/1000) 1.5321 2.4665 -0.2903 0.9563 0.1342 0.2876 ## ## ## Summary of Balance for Matched Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.5697 0.5692 0.0021 0.9848 0.0031 0.0546 ## age 25.6885 25.5410 0.0206 0.4839 0.0728 0.2404 ## educ 10.3224 10.6667 -0.1712 0.4586 0.0406 0.1475 ## black 0.8415 0.8361 0.0150 . 0.0055 0.0055 ## hispan 0.0601 0.0656 -0.0231 . 0.0055 0.0055 ## married 0.1913 0.1858 0.0140 . 0.0055 0.0055 ## I(re74/1000) 2.1185 2.3747 -0.0524 1.1054 0.0499 0.2896 ## I(re75/1000) 1.5058 1.5431 -0.0116 1.3188 0.0397 0.1967 ## Std. Pair Dist. ## distance 0.0112 ## age 1.0654 ## educ 1.0083 ## black 0.0451 ## hispan 0.2080 ## married 0.3767 ## I(re74/1000) 0.5800 ## I(re75/1000) 0.7106 ## ## Sample Sizes: ## Control Treated ## All 429. 185 ## Matched (ESS) 42.44 183 ## Matched 77. 183 ## Unmatched 278. 0 ## Discarded 74. 2 m.out$match.matrix m.out$distance plot(m.out\\(distance, m.out\\)fitted.values) # same method: exact, subclass, optimal, full, cem distance: pscore plot(m.out, type = “qq”, interactive=FALSE) 5.3.4 Checking balance on covariates: 5.3.4.1 Balance for formula 1: plot(summary(m.out)) 5.3.4.2 Balance for formula 2: set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm2, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = TRUE, caliper = 0.2, discard = &#39;both&#39; ) plot(summary(m.out)) love.plot(m.out, binary = &quot;std&quot;) Standardized mean differences (SMD): summary(m.out) ## ## Call: ## matchit(formula = fm2, data = lalonde, method = &quot;nearest&quot;, distance = &quot;logit&quot;, ## discard = &quot;both&quot;, replace = TRUE, caliper = 0.2) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.6237 0.1623 1.8005 1.3810 0.4018 0.6783 ## age 25.8162 28.0303 -0.3094 0.4400 0.0813 0.1577 ## I(age^2) 717.3946 901.7786 -0.4276 0.3627 0.0813 0.1577 ## I(age^3) 21554.6595 32892.1142 -0.5408 0.2882 0.0813 0.1577 ## educ 10.3459 10.2354 0.0550 0.4959 0.0347 0.1114 ## black 0.8432 0.2028 1.7615 . 0.6404 0.6404 ## hispan 0.0595 0.1422 -0.3498 . 0.0827 0.0827 ## married 0.1892 0.5128 -0.8263 . 0.3236 0.3236 ## I(re74/1000) 2.0956 5.6192 -0.7211 0.5181 0.2248 0.4470 ## I(re75/1000) 1.5321 2.4665 -0.2903 0.9563 0.1342 0.2876 ## ## ## Summary of Balance for Matched Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.5961 0.5956 0.0022 0.9866 0.0027 0.0533 ## age 25.5089 25.9763 -0.0653 0.9175 0.0419 0.1361 ## I(age^2) 705.0237 733.0651 -0.0650 0.9945 0.0419 0.1361 ## I(age^3) 21242.4320 22390.9941 -0.0548 1.0300 0.0419 0.1361 ## educ 10.4438 10.1775 0.1324 0.3762 0.0495 0.1834 ## black 0.8284 0.8580 -0.0814 . 0.0296 0.0296 ## hispan 0.0651 0.0473 0.0751 . 0.0178 0.0178 ## married 0.2071 0.2426 -0.0906 . 0.0355 0.0355 ## I(re74/1000) 2.2766 2.2802 -0.0007 1.2066 0.0364 0.2722 ## I(re75/1000) 1.5084 1.3610 0.0458 1.7749 0.0272 0.1657 ## Std. Pair Dist. ## distance 0.0117 ## age 1.1503 ## I(age^2) 1.0985 ## I(age^3) 1.0232 ## educ 1.3037 ## black 0.1790 ## hispan 0.3253 ## married 0.6647 ## I(re74/1000) 0.7456 ## I(re75/1000) 0.6802 ## ## Sample Sizes: ## Control Treated ## All 429. 185 ## Matched (ESS) 47.21 169 ## Matched 78. 169 ## Unmatched 229. 0 ## Discarded 122. 16 #out = summary(m.out) #round(out$sum.all, 3) #round(out$sum.matched, 3) plot(m.out, type = &quot;hist&quot;, interactive = F) plot(m.out, type = &quot;jitter&quot;, interactive = F) love.plot(m.out, binary = &quot;std&quot;) bal.plot(m.out, var.name = &quot;distance&quot;, which = &quot;both&quot;, type = &quot;histogram&quot;, mirror = TRUE) who matched to whom? head(m.out$match.matrix, 10) ## [,1] ## NSW1 &quot;PSID15&quot; ## NSW2 &quot;PSID76&quot; ## NSW3 NA ## NSW4 &quot;PSID356&quot; ## NSW5 NA ## NSW6 &quot;PSID269&quot; ## NSW7 &quot;PSID269&quot; ## NSW8 &quot;PSID356&quot; ## NSW9 &quot;PSID253&quot; ## NSW10 &quot;PSID329&quot; 5.3.4.3 Balance for formula 3: set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm3, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = FALSE, caliper = 0.2, discard = &#39;both&#39; ) summary(m.out) ## ## Call: ## matchit(formula = fm3, data = lalonde, method = &quot;nearest&quot;, distance = &quot;logit&quot;, ## discard = &quot;both&quot;, replace = FALSE, caliper = 0.2) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.6434 0.1538 1.8536 1.5312 0.4089 0.6923 ## age 25.8162 28.0303 -0.3094 0.4400 0.0813 0.1577 ## I(age^2) 717.3946 901.7786 -0.4276 0.3627 0.0813 0.1577 ## I(age^3) 21554.6595 32892.1142 -0.5408 0.2882 0.0813 0.1577 ## educ 10.3459 10.2354 0.0550 0.4959 0.0347 0.1114 ## I(educ^2) 111.0595 112.8974 -0.0468 0.5173 0.0347 0.1114 ## black 0.8432 0.2028 1.7615 . 0.6404 0.6404 ## hispan 0.0595 0.1422 -0.3498 . 0.0827 0.0827 ## married 0.1892 0.5128 -0.8263 . 0.3236 0.3236 ## I(re74/1000) 2.0956 5.6192 -0.7211 0.5181 0.2248 0.4470 ## I(re75/1000) 1.5321 2.4665 -0.2903 0.9563 0.1342 0.2876 ## ## ## Summary of Balance for Matched Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.4892 0.4638 0.0961 1.1554 0.0146 0.10 ## age 25.4100 24.7600 0.0908 0.9874 0.0238 0.09 ## I(age^2) 713.2500 681.5000 0.0736 0.9455 0.0238 0.09 ## I(age^3) 22175.6700 21050.3800 0.0537 0.8639 0.0238 0.09 ## educ 10.2100 10.4000 -0.0945 0.9000 0.0163 0.09 ## I(educ^2) 109.0500 113.5000 -0.1132 0.9644 0.0163 0.09 ## black 0.7200 0.7300 -0.0275 . 0.0100 0.01 ## hispan 0.1000 0.0800 0.0846 . 0.0200 0.02 ## married 0.2500 0.2400 0.0255 . 0.0100 0.01 ## I(re74/1000) 2.8664 3.0329 -0.0341 1.5574 0.0691 0.29 ## I(re75/1000) 1.9818 2.1229 -0.0438 1.2273 0.0441 0.15 ## Std. Pair Dist. ## distance 0.0993 ## age 0.9602 ## I(age^2) 0.9191 ## I(age^3) 0.8759 ## educ 1.1190 ## I(educ^2) 1.1640 ## black 0.4676 ## hispan 0.7612 ## married 0.7404 ## I(re74/1000) 0.9759 ## I(re75/1000) 1.0367 ## ## Sample Sizes: ## Control Treated ## All 429 185 ## Matched 100 100 ## Unmatched 208 77 ## Discarded 121 8 plot(summary(m.out)) love.plot(m.out, binary = &quot;std&quot;) 5.3.5 Estimation of treatment effect I will use the matching results with fm3 because it yielded the best balance on the covariates between the matched groups. I also kept ratio = 1 and replace = FALSE to avoid the specification of weights for now: set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm3, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = TRUE, ratio = 1, caliper = 0.2, discard = &#39;both&#39;, estimand = &#39;ATC&#39; ) plot(summary(m.out)) love.plot(m.out, binary = &quot;std&quot;) Extract matched data: m.data &lt;- match.data(m.out) # m.data &lt;- get_matches(m.out) tail(m.data) ## treat age educ race married nodegree re74 re75 re78 black hispan un74 ## PSID419 0 51 4 black 0 1 0 0 0.0000 1 0 1 ## PSID420 0 39 2 black 1 1 0 0 964.9555 1 0 1 ## PSID423 0 27 10 black 0 1 0 0 7543.7940 1 0 1 ## PSID424 0 25 14 white 0 0 0 0 0.0000 0 0 1 ## PSID425 0 18 11 white 0 1 0 0 10150.5000 0 0 1 ## PSID428 0 32 5 black 1 1 0 0 187.6713 1 0 1 ## un75 pscore distance weights ## PSID419 1 0.36518898 0.19705592 1 ## PSID420 1 0.59738576 0.09606209 1 ## PSID423 1 0.90547079 0.94416481 1 ## PSID424 1 0.23858002 0.13851860 1 ## PSID425 1 0.05225581 0.05590214 1 ## PSID428 1 0.75439262 0.62639135 1 dim(m.data) ## [1] 376 16 5.3.5.1 Linear model without covariates: fit1 &lt;- lm(re78 ~ treat, data = m.data) summ(fit1) ## MODEL INFO: ## Observations: 376 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(1,374) = 0.24, p = 0.62 ## R² = 0.00 ## Adj. R² = -0.00 ## ## Standard errors: OLS ## ---------------------------------------------------- ## Est. S.E. t val. p ## ----------------- --------- -------- -------- ------ ## (Intercept) 6178.93 377.96 16.35 0.00 ## treat 438.99 888.75 0.49 0.62 ## ---------------------------------------------------- Cluster-robust standard errors: coeftest(fit1, vcov. = vcovCL) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6178.93 379.00 16.3032 &lt;2e-16 *** ## treat 438.99 880.03 0.4988 0.6182 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Alternatively, use weighted Student’s t-test: res &lt;- wtd.t.test(m.data$re78[m.data$treat == 1], m.data$re78[m.data$treat == 0], weight = m.data$weights[m.data$treat == 1], weighty = m.data$weights[m.data$treat == 0]) ## Warning in wtd.t.test(m.data$re78[m.data$treat == 1], m.data$re78[m.data$treat == : ## Treating data for x and y separately because they are of different lengths print(res) ## $test ## [1] &quot;Two Sample Weighted T-Test (Welch)&quot; ## ## $coefficients ## t.value df p.value ## 1.6213231 89.4080655 0.1084699 ## ## $additional ## Difference Mean.x Mean.y Std. Err ## 1660.279 7839.210 6178.931 1024.027 mu &lt;- res$additional[1] std &lt;- res$additional[4] cat(&quot;Confidence interval: &quot;, sapply(qt(c(0.025, 0.975), coef(res)[&quot;df&quot;]), function(x){return(mu+x*std)}), &quot;\\n&quot;) ## Confidence interval: -374.3133 3694.871 5.3.5.2 Linear model with covariates: double robust (use with caution) fit2 &lt;- lm(re78 ~ treat + age + educ + black + hispan + married + nodegree + un74 + un75 + re74 + re75, data = m.data) # include only the covariate that is not balanced fit2 &lt;- lm(re78 ~ treat + I(educ^2), data = m.data) summ(fit2) ## MODEL INFO: ## Observations: 376 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(2,373) = 3.04, p = 0.05 ## R² = 0.02 ## Adj. R² = 0.01 ## ## Standard errors: OLS ## ---------------------------------------------------- ## Est. S.E. t val. p ## ----------------- --------- -------- -------- ------ ## (Intercept) 4225.52 891.47 4.74 0.00 ## treat 419.16 883.10 0.47 0.64 ## I(educ^2) 17.50 7.24 2.42 0.02 ## ---------------------------------------------------- coeftest(fit2, vcov. = vcovCL) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4225.5181 902.7059 4.6809 4.003e-06 *** ## treat 419.1608 861.5084 0.4865 0.62687 ## I(educ^2) 17.5011 7.4731 2.3419 0.01971 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.3.6 Hidden bias analysis extract matched treatment and control units from m.data psens(x = m.data$re78[m.data$treat==1], y = m.data$re78[m.data$treat==0], Gamma = 3, GammaInc=0.1) ## Warning in trt - ctrl: longer object length is not a multiple of shorter object length ## ## Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value ## ## Unconfounded estimate .... 0.1886 ## ## Gamma Lower bound Upper bound ## 1.0 0.1886 0.1886 ## 1.1 0.0556 0.4303 ## 1.2 0.0124 0.6808 ## 1.3 0.0022 0.8564 ## 1.4 0.0003 0.9470 ## 1.5 0.0000 0.9835 ## 1.6 0.0000 0.9956 ## 1.7 0.0000 0.9989 ## 1.8 0.0000 0.9998 ## 1.9 0.0000 1.0000 ## 2.0 0.0000 1.0000 ## 2.1 0.0000 1.0000 ## 2.2 0.0000 1.0000 ## 2.3 0.0000 1.0000 ## 2.4 0.0000 1.0000 ## 2.5 0.0000 1.0000 ## 2.6 0.0000 1.0000 ## 2.7 0.0000 1.0000 ## 2.8 0.0000 1.0000 ## 2.9 0.0000 1.0000 ## 3.0 0.0000 1.0000 ## ## Note: Gamma is Odds of Differential Assignment To ## Treatment Due to Unobserved Factors ## a gamma value with 1.6 or larger could lead to a change in the ATT estimate. 5.3.7 References: matchit: https://kosukeimai.github.io/MatchIt/articles/MatchIt.html manual: https://imai.fas.harvard.edu/research/files/matchit.pdf https://cran.r-project.org/web/packages/MatchIt/vignettes/assessing-balance.html starting from page 15 Let’s play with the arguments! "],["propensity-score-analysis.html", "Chapter 6 Propensity Score Analysis 6.1 Data 6.2 Calculating Propensity Scores 6.3 IPTW: Inverse Probability Treatment Weighting 6.4 Stratification 6.5 Adjustment 6.6 Exercise", " Chapter 6 Propensity Score Analysis In this lab, we will use propensity scores to perform other types of analyses (weighting, stratification, and covariate adjustment) library(weights) library(survey) library(twang) library(CBPS) library(cobalt) library(jtools) library(lmtest) library(sandwich) #vcovCL library(rbounds) #gamma library(tidyr) library(tidyverse) library(janitor) #remotes::install_github(&quot;vdorie/treatSens&quot;) #library(treatSens) 6.1 Data #data(lalonde) lalonde = MatchIt::lalonde dim(lalonde) ## [1] 614 9 names(lalonde) ## [1] &quot;treat&quot; &quot;age&quot; &quot;educ&quot; &quot;race&quot; &quot;married&quot; &quot;nodegree&quot; &quot;re74&quot; ## [8] &quot;re75&quot; &quot;re78&quot; 6.1.1 Create dummy variable for race and unemployment lalonde$black = ifelse(lalonde$race==&#39;black&#39;, 1, 0) lalonde$hispan = ifelse(lalonde$race==&#39;hispan&#39;, 1, 0) lalonde$un74 = ifelse(lalonde$re74==0, 1, 0) lalonde$un75 = ifelse(lalonde$re75==0, 1, 0) 6.2 Calculating Propensity Scores 6.2.1 Different formulas fm1 = treat ~ age + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm2 = treat ~ age + I(age^2) + I(age^3) + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm3 = treat ~ age + I(age^2) + I(age^3) + educ + I(educ^2) + black + hispan + married + I(re74/1000) + I(re75/1000) 6.2.2 Calculation of propensity scores (p-scores) pscore &lt;- glm(fm3, data = lalonde, family = &#39;binomial&#39;) head(pscore$fitted.values) ## NSW1 NSW2 NSW3 NSW4 NSW5 NSW6 ## 0.6664569 0.3896185 0.9237414 0.9345017 0.9434150 0.8771158 hist(pscore$fitted.values[lalonde$treat==0],xlim=c(0,1)) hist(pscore$fitted.values[lalonde$treat==1],xlim=c(0,1), add=F) lalonde$pscore = pscore$fitted.values hist(pscore$fitted.values[lalonde$treat==0],xlim=c(0,1), density = 20, angle = 45, main=&quot;Propensity Scores&quot;, xlab=&quot;Shaded = Untreated | Gray = Treated&quot;) hist(pscore$fitted.values[lalonde$treat==1],xlim=c(0,1), col=gray(0.4,0.25),add=T) 6.3 IPTW: Inverse Probability Treatment Weighting 6.3.1 weightATT weightATT is created by using the ifelse() function to obtain: * 1 for treated units * p/(1-p) for control units lalonde$weightATT &lt;- with(lalonde, ifelse(treat==1, 1, pscore/(1-pscore))) A summary of the ATT weights for treated and untreated groups: with(lalonde, by(pscore,treat,summary)) ## treat: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000492 0.0241907 0.0611922 0.1537662 0.1617857 0.9441648 ## ----------------------------------------------------------------- ## treat: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.02584 0.47624 0.69596 0.64343 0.87791 0.95468 with(lalonde, by(weightATT,treat,summary)) ## treat: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000049 0.024790 0.065181 0.444293 0.193012 16.909853 ## ----------------------------------------------------------------- ## treat: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 boxplot(lalonde$weightATT ~ lalonde$treat) 6.3.2 weightATE weightATE is created by using the ifelse() function to obtain: * 1/p for treated units * 1/(1-p) for control units lalonde$weightATE &lt;- with(lalonde, ifelse(treat==1, 1/pscore, 1/(1-pscore))) A summary of the ATE weights for treated and untreated groups: with(lalonde, by(weightATE,treat,summary)) ## treat: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.025 1.065 1.444 1.193 17.910 ## ----------------------------------------------------------------- ## treat: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.047 1.139 1.437 3.015 2.100 38.703 boxplot(lalonde$weightATE ~ lalonde$treat) the maximum weight for the ATE is 33.961 6.3.3 Weight truncation Truncation can be performed by assigning the weight at a cutoff percentile to observations with weights above the cutoff. The following code demonstrates weight truncation. More specifically, it uses the quantile function to calculate the weight at the 99th percentile and the ifelse function to assign this weight to any student whose weight exceeds the 99th percentile: lalonde$weightATETruncated &lt;- with(lalonde, ifelse(weightATE &gt; quantile(weightATE, 0.99), quantile(weightATE, 0.99), weightATE)) with(lalonde, by(weightATETruncated,treat,summary)) ## treat: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.025 1.065 1.440 1.193 15.888 ## ----------------------------------------------------------------- ## treat: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.047 1.139 1.437 2.571 2.100 15.888 boxplot(lalonde$weightATETruncated ~ lalonde$treat) may truncate all above .05 quantiles if necessary with(lalonde, by(weightATETruncated,treat,summary)) ## treat: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.025 1.065 1.440 1.193 15.888 ## ----------------------------------------------------------------- ## treat: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.047 1.139 1.437 2.571 2.100 15.888 boxplot(lalonde$weightATETruncated ~ lalonde$treat) 6.3.4 Balance check In this section, covariate balance evaluation is performed by comparing the standardized difference between the weighted means of the treated and untreated groups. The bal.stat function of the twang package (Ridgeway et al., 2013) is useful for covariate balance evaluation. If there are no sampling weights, then sampw=1. covariateNames &lt;-names(lalonde)[2:14] balanceTable &lt;- bal.stat(lalonde, vars= covariateNames, treat.var = &quot;treat&quot;, w.all = lalonde$weightATE, get.ks=F, sampw = 1, estimand=&quot;ATE&quot;, multinom=F) balanceTable &lt;- balanceTable$results round(balanceTable,3) ## tx.mn tx.sd ct.mn ct.sd std.eff.sz stat p ## age 24.637 7.520 27.512 9.921 -0.291 -2.608 0.009 ## educ 10.653 2.225 10.323 2.613 0.125 0.905 0.366 ## race:black 0.479 0.500 0.403 0.490 0.157 2.651 0.072 ## race:hispan 0.222 0.416 0.117 0.321 0.326 NA NA ## race:white 0.299 0.458 0.480 0.500 -0.363 NA NA ## married 0.403 0.492 0.403 0.491 0.000 0.000 1.000 ## nodegree 0.627 0.485 0.592 0.492 0.072 0.428 0.668 ## re74 4423.792 8491.218 4470.490 6258.399 -0.007 -0.026 0.979 ## re75 2298.397 3691.811 2121.943 3124.212 0.054 0.271 0.786 ## re78 7624.783 9058.957 6307.898 6870.411 0.176 0.743 0.458 ## black 0.479 0.501 0.403 0.491 0.156 0.921 0.358 ## hispan 0.222 0.417 0.117 0.322 0.326 1.244 0.214 ## un74 0.545 0.499 0.336 0.473 0.427 2.403 0.017 ## un75 0.461 0.500 0.374 0.484 0.179 1.071 0.285 ## pscore 0.332 0.322 0.308 0.327 0.075 0.438 0.661 std.eff.sz quantifies effect size 0.2 small 0.5 medium 0.8 large most of them are small (except nodegree and un74) - balance was somewhat achieved * 6.3.5 Estimation of treatment effect the final weights are divided by the mean of weights to make them sum to the sample size, which is a process known as normalization. lalonde$finalWeight &lt;- lalonde$weightATE/mean(lalonde$weightATE) Before the estimation can be performed, the surveyDesign object is created with the svydesign function to declare the names of the variables that contain cluster ids, strata ids, weights, and the data set: surveyDesign &lt;- svydesign(ids=~1, weights=~finalWeight, data = lalonde, nest=T) Methods to obtain standard errors for propensity score weighted estimates include Taylor series linearization and resampling methods such as bootstrapping, jackknife, and balanced repeated replication (see review by Rodgers, 1999). To use bootstrap methods with the survey package, the surveyDesign object created above should be modified to include weights for each replication. The following code takes the surveyDesign object and adds weights for 1,000 bootstrapped samples: set.seed(8) surveyDesignBoot &lt;- as.svrepdesign(surveyDesign, type=c(&quot;bootstrap&quot;), replicates=1000) First, the svyby function is used to apply the svymean function separately to treated and control units to obtain weighted outcome: weightedMeans &lt;- svyby(formula=~re78, by=~treat, design=surveyDesignBoot, FUN=svymean, covmat=TRUE) weightedMeans ## treat re78 se ## 0 0 6307.898 392.3048 ## 1 1 7624.783 1715.2247 The code to obtain the treatment effect with a regression model is shown as follows. The model formula using R notation is re78~treat. This formula can be expanded to include any covariates and interaction effects of interest outcomeModel &lt;- svyglm(re78~treat, design = surveyDesignBoot) summary(outcomeModel) ## ## Call: ## svyglm(formula = re78 ~ treat, design = surveyDesignBoot) ## ## Survey design: ## as.svrepdesign.default(surveyDesign, type = c(&quot;bootstrap&quot;), replicates = 1000) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6307.9 392.3 16.079 &lt;2e-16 *** ## treat 1316.9 1753.6 0.751 0.453 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 38958653771) ## ## Number of Fisher Scoring iterations: 2 Double robust with the imbalanced covariates: outcomeModel &lt;- svyglm(re78 ~ treat + age + un74 + hispan, design = surveyDesignBoot) summary(outcomeModel) ## ## Call: ## svyglm(formula = re78 ~ treat + age + un74 + hispan, design = surveyDesignBoot) ## ## Survey design: ## as.svrepdesign.default(surveyDesign, type = c(&quot;bootstrap&quot;), replicates = 1000) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5994.60 1346.99 4.450 1.02e-05 *** ## treat 2175.86 2182.05 0.997 0.319 ## age 50.02 34.22 1.462 0.144 ## un74 -2567.20 1952.86 -1.315 0.189 ## hispan -1708.72 1719.49 -0.994 0.321 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 37748563384) ## ## Number of Fisher Scoring iterations: 2 6.4 Stratification 6.4.1 Manually creating subclasses The following code used the cut function to create five strata of approximately the same size based on the quintiles of the distribution of propensity scores for both treated and untreated groups. The quintiles are obtained with the quantile function. The function levels is used to assign number labels from 1 to 5 to the strata, and then xtabs is used to display strata by treatment counts. The number of strata is limited by the common support of the propensity score distributions of treated and untreated groups, because each stratum must have at least one treated and one untreated observation. hist(lalonde$pscore) quantile(lalonde$pscore, prob = seq(0, 1, 1/5)) ## 0% 20% 40% 60% 80% 100% ## 4.922848e-05 2.608815e-02 7.289915e-02 2.419414e-01 6.698725e-01 9.546820e-01 lalonde$subclass &lt;- cut(x=lalonde$pscore, breaks = quantile(lalonde$pscore, prob = seq(0, 1, 1/5)), include.lowest=T) levels(lalonde$subclass) &lt;- 1:length(levels(lalonde$subclass)) ntable &lt;- xtabs(~treat+subclass,lalonde) ntable ## subclass ## treat 1 2 3 4 5 ## 0 122 118 106 61 22 ## 1 1 5 16 62 101 surveyDesign &lt;- svydesign(ids=~1, weights=~lalonde$finalWeight, data = lalonde, nest=T) 6.4.2 Stratification using matchit(): set.seed(42) stratification &lt;- matchit(data = lalonde, formula = fm2, distance = &quot;logit&quot;, method = &quot;subclass&quot;, subclass = 5) 6.4.3 Covariate Balance Evaluation balance.stratification = summary(stratification) balance.stratification$qn ## 1 2 3 4 5 All ## Control 364 35 14 11 5 429 ## Treated 35 39 37 37 37 185 ## Total 399 74 51 48 42 614 It is noticeable in the cross-classification of treatment by strata shown above that the stratification based on the propensity scores of the treated resulted in a similar number of treated units within strata. When using summary(), the default is to display balance only in aggregate using the subclassification weights. This balance output looks similar to that for other matching methods. round(balance.stratification$sum.across,3) ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.624 0.593 0.121 0.803 0.047 ## age 25.816 25.865 -0.007 0.811 0.030 ## `I(age^2)` 717.395 731.133 -0.032 0.797 0.030 ## `I(age^3)` 21554.659 22635.660 -0.052 0.751 0.030 ## educ 10.346 10.386 -0.020 0.389 0.045 ## black 0.843 0.811 0.088 NA 0.032 ## hispan 0.059 0.043 0.071 NA 0.017 ## married 0.189 0.212 -0.058 NA 0.023 ## `I(re74/1000)` 2.096 2.585 -0.100 0.953 0.047 ## `I(re75/1000)` 1.532 1.542 -0.003 1.311 0.024 ## eCDF Max Std. Pair Dist. ## distance 0.098 NA ## age 0.112 NA ## `I(age^2)` 0.112 NA ## `I(age^3)` 0.112 NA ## educ 0.178 NA ## black 0.032 NA ## hispan 0.017 NA ## married 0.023 NA ## `I(re74/1000)` 0.252 NA ## `I(re75/1000)` 0.101 NA If the goal is to estimate the treatment effect by pooling stratum-specific treatment effects, covariate balance should be evaluated and achieved within strata. * However, if the number of covariates is large, evaluation of covariate balance within strata can become cumbersome. Also, if the sample sizes of treated or untreated groups within strata are small, covariate balance evaluation can become very sensitive to outliers. An additional option in summary(), subclass, allows us to request balance for individual subclasses. Below we call summary() and request balance to be displayed on all subclasses (setting un = FALSE to suppress balance in the original sample): summary(stratification, subclass = TRUE, un = FALSE) ## ## Call: ## matchit(formula = fm2, data = lalonde, method = &quot;subclass&quot;, distance = &quot;logit&quot;, ## subclass = 5) ## ## Summary of Balance by Subclass: ## ## - Subclass 1 ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.1939 0.0801 0.4443 1.9143 0.3036 ## age 26.5714 28.8626 -0.3202 0.6086 0.0736 ## `I(age^2)` 779.3143 956.6593 -0.4112 0.5202 0.0736 ## `I(age^3)` 25222.1143 35807.3242 -0.5049 0.4174 0.0736 ## educ 10.5143 10.2170 0.1478 0.5463 0.0284 ## black 0.2571 0.0659 0.5259 . 0.1912 ## hispan 0.2286 0.1621 0.2811 . 0.0665 ## married 0.3714 0.5742 -0.5177 . 0.2027 ## `I(re74/1000)` 3.9016 6.2471 -0.4800 1.1744 0.1693 ## `I(re75/1000)` 2.1251 2.6298 -0.1568 0.9425 0.0761 ## eCDF Max ## distance 0.5192 ## age 0.1527 ## `I(age^2)` 0.1527 ## `I(age^3)` 0.1527 ## educ 0.0940 ## black 0.1912 ## hispan 0.0665 ## married 0.2027 ## `I(re74/1000)` 0.3720 ## `I(re75/1000)` 0.1890 ## ## - Subclass 2 ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.5145 0.5169 -0.0093 1.0639 0.0412 ## age 23.9231 21.6857 0.3127 1.6826 0.0757 ## `I(age^2)` 658.7949 521.5143 0.3183 1.8594 0.0757 ## `I(age^3)` 20941.4615 14232.2571 0.3200 2.0927 0.0757 ## educ 9.3846 9.9143 -0.2634 0.4968 0.0546 ## black 0.9231 0.9429 -0.0544 . 0.0198 ## hispan 0.0769 0.0571 0.0836 . 0.0198 ## married 0.2308 0.2000 0.0786 . 0.0308 ## `I(re74/1000)` 2.3422 2.0878 0.0521 1.9796 0.0920 ## `I(re75/1000)` 2.5483 1.5986 0.2950 3.2557 0.0886 ## eCDF Max ## distance 0.1443 ## age 0.1619 ## `I(age^2)` 0.1619 ## `I(age^3)` 0.1619 ## educ 0.2549 ## black 0.0198 ## hispan 0.0198 ## married 0.0308 ## `I(re74/1000)` 0.2894 ## `I(re75/1000)` 0.2381 ## ## - Subclass 3 ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.6695 0.6605 0.0351 1.2277 0.0945 ## age 25.0541 23.0714 0.2771 1.8144 0.0760 ## `I(age^2)` 679.1622 559.3571 0.2778 2.6006 0.0760 ## `I(age^3)` 20134.6216 14283.3571 0.2791 3.9868 0.0760 ## educ 10.8378 11.2857 -0.2228 0.2867 0.0731 ## black 1.0000 1.0000 0.0000 . 0.0000 ## hispan 0.0000 0.0000 0.0000 . 0.0000 ## married 0.3243 0.2143 0.2810 . 0.1100 ## `I(re74/1000)` 2.2776 3.7009 -0.2913 0.4103 0.0866 ## `I(re75/1000)` 1.2027 2.5788 -0.4275 0.1933 0.1333 ## eCDF Max ## distance 0.1931 ## age 0.2375 ## `I(age^2)` 0.2375 ## `I(age^3)` 0.2375 ## educ 0.3456 ## black 0.0000 ## hispan 0.0000 ## married 0.1100 ## `I(re74/1000)` 0.2375 ## `I(re75/1000)` 0.3031 ## ## - Subclass 4 ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.8194 0.7872 0.1256 0.8049 0.1987 ## age 25.9730 26.0909 -0.0165 0.6325 0.0880 ## `I(age^2)` 704.1892 724.4545 -0.0470 0.6692 0.0880 ## `I(age^3)` 20057.2162 21441.1818 -0.0660 0.7053 0.0880 ## educ 10.8108 10.7273 0.0415 0.4211 0.0781 ## black 1.0000 1.0000 0.0000 . 0.0000 ## hispan 0.0000 0.0000 0.0000 . 0.0000 ## married 0.0270 0.0909 -0.1631 . 0.0639 ## `I(re74/1000)` 1.9019 1.0422 0.1759 3.4387 0.0793 ## `I(re75/1000)` 0.9004 0.6699 0.0716 1.9497 0.0662 ## eCDF Max ## distance 0.3735 ## age 0.2285 ## `I(age^2)` 0.2285 ## `I(age^3)` 0.2285 ## educ 0.2187 ## black 0.0000 ## hispan 0.0000 ## married 0.0639 ## `I(re74/1000)` 0.2482 ## `I(re75/1000)` 0.1351 ## ## - Subclass 5 ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.9037 0.8952 0.0333 2.0591 0.2208 ## age 27.7027 30.0000 -0.3211 0.2193 0.2015 ## `I(age^2)` 772.0270 917.2000 -0.3366 0.1871 0.2015 ## `I(age^3)` 21649.2703 28580.4000 -0.3306 0.1596 0.2015 ## educ 10.2432 9.8000 0.2204 0.2171 0.1120 ## black 1.0000 1.0000 0.0000 . 0.0000 ## hispan 0.0000 0.0000 0.0000 . 0.0000 ## married 0.0000 0.0000 0.0000 . 0.0000 ## `I(re74/1000)` 0.1388 0.0697 0.0141 10.1413 0.0562 ## `I(re75/1000)` 0.8609 0.2897 0.1774 8.8591 0.0752 ## eCDF Max ## distance 0.4486 ## age 0.4000 ## `I(age^2)` 0.4000 ## `I(age^3)` 0.4000 ## educ 0.3838 ## black 0.0000 ## hispan 0.0000 ## married 0.0000 ## `I(re74/1000)` 0.1189 ## `I(re75/1000)` 0.1892 ## ## Sample Sizes by Subclass: ## 1 2 3 4 5 All ## Control 364 35 14 11 5 429 ## Treated 35 39 37 37 37 185 ## Total 399 74 51 48 42 614 We can plot the standardized mean differences in a Love plot that also displays balance for the subclasses using plot.summary.matchit() on a summary.matchit() object with subclass = TRUE. balance.stratification2 &lt;- summary(stratification, subclass = TRUE) plot(balance.stratification2, var.order = &quot;unmatched&quot;, abs = FALSE) Note that for some variables, while the groups are balanced in aggregate (black dots), the individual subclasses (gray numbers) may not be balanced, in which case unadjusted effect estimates within these subclasses should not be interpreted as unbiased. 6.4.4 Calculate the stratum weights To use bootstrap methods with the survey package, the surveyDesign object created above should be modified to include weights for each replication. The following code takes the surveyDesign object and adds weights for 1,000 bootstrapped samples: set.seed(8) surveyDesignBoot &lt;- as.svrepdesign(surveyDesign, type=c(&quot;bootstrap&quot;), replicates=1000) The following R code uses svyby to apply the svymean function: head(lalonde) ## treat age educ race married nodegree re74 re75 re78 black hispan un74 ## NSW1 1 37 11 black 1 1 0 0 9930.0460 1 0 1 ## NSW2 1 22 9 hispan 0 1 0 0 3595.8940 0 1 1 ## NSW3 1 30 12 black 0 0 0 0 24909.4500 1 0 1 ## NSW4 1 27 11 black 0 1 0 0 7506.1460 1 0 1 ## NSW5 1 33 8 black 0 1 0 0 289.7899 1 0 1 ## NSW6 1 22 9 black 0 1 0 0 4056.4940 1 0 1 ## un75 pscore weightATT weightATE weightATETruncated finalWeight subclass ## NSW1 1 0.6664569 1 1.500472 1.500472 0.7824785 4 ## NSW2 1 0.3896185 1 2.566613 2.566613 1.3384583 4 ## NSW3 1 0.9237414 1 1.082554 1.082554 0.5645392 5 ## NSW4 1 0.9345017 1 1.070089 1.070089 0.5580388 5 ## NSW5 1 0.9434150 1 1.059979 1.059979 0.5527665 5 ## NSW6 1 0.8771158 1 1.140100 1.140100 0.5945489 5 subclassMeans &lt;- svyby(formula=~re78, by=~treat+subclass, design=surveyDesignBoot, FUN=svymean, covmat=TRUE) ## Warning in svrVar(repmeans, scale, rscales, mse = design$mse, coef = rval): 350 ## replicates gave NA results and were discarded. ## Warning in svrVar(repmeans, scale, rscales, mse = design$mse, coef = rval): 5 ## replicates gave NA results and were discarded. ## Warning in svrVar(replicates, design$scale, design$rscales, mse = design$mse, : 354 ## replicates gave NA results and were discarded. subclassMeans ## treat subclass re78 se ## 0.1 0 1 9053.491 7.538916e+02 ## 1.1 1 1 6788.463 4.072737e-14 ## 0.2 0 2 6911.435 6.427576e+02 ## 1.2 1 2 10143.806 7.484989e+03 ## 0.3 0 3 6380.553 6.654576e+02 ## 1.3 1 3 8460.492 1.152910e+03 ## 0.4 0 4 4663.733 8.119789e+02 ## 1.4 1 4 5474.848 7.412451e+02 ## 0.5 0 5 4578.930 1.080237e+03 ## 1.5 1 5 6396.858 8.615546e+02 To obtain the ATE or ATT by pooling stratum-specific effects, svycontrast is used with the weights First, use ntable to obtain the weights: For estimating the ATE, the stratum weight is wk = nk/n, which is the stratum size divided by the total sample size. For estimating the ATT, the stratum weight is, wk = n1k/n1 which is the treated sample size within the stratum divided by the total treated sample size. subclass_table = balance.stratification2$qn[1:2,1:5] ATEw = colSums(subclass_table)/sum(subclass_table) ATTw = subclass_table[2,]/sum(subclass_table[2,]) subclass_table; ATEw; ATTw ## 1 2 3 4 5 ## Control 364 35 14 11 5 ## Treated 35 39 37 37 37 ## 1 2 3 4 5 ## 0.64983713 0.12052117 0.08306189 0.07817590 0.06840391 ## 1 2 3 4 5 ## 0.1891892 0.2108108 0.2000000 0.2000000 0.2000000 This won’t work: pooledEffects &lt;- svycontrast(subclassMeans, contrasts = list(ATT=ATTw)) ATEw and ATTw needs to be specified for EVERY group within EVERY stratum in our case, 10 groups (2*5 = 10) Control groups get negative weights and treatment group gets positive weights: subclassMeans$ATEsw &lt;- c(-ATEw[1], ATEw[1], # first stratum, -ATEw[2], ATEw[2], # second stratum -ATEw[3], ATEw[3], # third stratum -ATEw[4], ATEw[4], # fourth stratum -ATEw[4], ATEw[5]) # fifth stratum subclassMeans$ATTsw &lt;- c(-ATTw[1], ATTw[1], # first stratum, -ATTw[2], ATTw[2], # second stratum -ATTw[3], ATTw[3], # third stratum -ATTw[4], ATTw[4], # fourth stratum -ATTw[4], ATTw[5]) # fifth stratum subclassMeans ## treat subclass re78 se ATEsw ATTsw ## 0.1 0 1 9053.491 7.538916e+02 -0.64983713 -0.1891892 ## 1.1 1 1 6788.463 4.072737e-14 0.64983713 0.1891892 ## 0.2 0 2 6911.435 6.427576e+02 -0.12052117 -0.2108108 ## 1.2 1 2 10143.806 7.484989e+03 0.12052117 0.2108108 ## 0.3 0 3 6380.553 6.654576e+02 -0.08306189 -0.2000000 ## 1.3 1 3 8460.492 1.152910e+03 0.08306189 0.2000000 ## 0.4 0 4 4663.733 8.119789e+02 -0.07817590 -0.2000000 ## 1.4 1 4 5474.848 7.412451e+02 0.07817590 0.2000000 ## 0.5 0 5 4578.930 1.080237e+03 -0.07817590 -0.2000000 ## 1.5 1 5 6396.858 8.615546e+02 0.06840391 0.2000000 6.4.5 Estimation of Treatment Effects view ATTw2 here: pooledEffects &lt;- svycontrast(subclassMeans, list(ATE = subclassMeans$ATTsw, ATT = subclassMeans$ATEsw)) pooledEffects ## contrast SE ## ATE 1194.70 1311.46 ## ATT -766.55 844.72 6.5 Adjustment adjustModel &lt;- lm(re78~treat+pscore,lalonde) summ(adjustModel) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(2,611) = 4.44, p = 0.01 ## R² = 0.01 ## Adj. R² = 0.01 ## ## Standard errors: OLS ## ------------------------------------------------------ ## Est. S.E. t val. p ## ----------------- ---------- --------- -------- ------ ## (Intercept) 7549.86 411.09 18.37 0.00 ## treat 1166.40 914.38 1.28 0.20 ## pscore -3678.92 1306.24 -2.82 0.01 ## ------------------------------------------------------ 6.6 Exercise Exercise: can you use fm2 to obtain propensity scores and see if that improves the balance and thus the estimate of treatment effect? "],["difference-in-differences-did.html", "Chapter 7 Difference-in-differences (DID) 7.1 Data 7.2 The DID estimator 7.3 Card and Krueger (1994) from PoEdata 7.4 Demo of HonestDiD (Rambachan &amp; Roth, 2022) 7.5 Synthetic Control", " Chapter 7 Difference-in-differences (DID) In this lab, we will use propensity scores to perform other types of analyses (weighting, stratification, and covariate adjustment) library(foreign) library(car) library(stargazer) #library(devtools) #devtools::install_github(&#39;ccolonescu/PoEdata&#39;) library(PoEdata) 7.1 Data Getting sample data: mydata = read.dta(&quot;http://dss.princeton.edu/training/Panel101.dta&quot;) head(mydata) ## country year y y_bin x1 x2 x3 opinion op ## 1 A 1990 1342787840 1 0.2779036 -1.1079559 0.28255358 Str agree 1 ## 2 A 1991 -1899660544 0 0.3206847 -0.9487200 0.49253848 Disag 0 ## 3 A 1992 -11234363 0 0.3634657 -0.7894840 0.70252335 Disag 0 ## 4 A 1993 2645775360 1 0.2461440 -0.8855330 -0.09439092 Disag 0 ## 5 A 1994 3008334848 1 0.4246230 -0.7297683 0.94613063 Disag 0 ## 6 A 1995 3229574144 1 0.4772141 -0.7232460 1.02968037 Str agree 1 7.1.1 Scatterplot by country: scatterplot(y~year|country, boxplots=FALSE, smooth=TRUE, data = mydata) 7.1.2 Create dummy variables time: time when the treatment started Let’s assume that treatment started in 1994. In this case, years before 1994 will have a value of 0 and 1994+ a 1. If you already have this skip this step. mydata$time = ifelse(mydata$year &gt;= 1994, 1, 0) 2.treated: the group exposed to the treatment In this example let’s assumed that countries with code 5,6, and 7 were treated (=1). Countries 1-4 were not treated (=0). If you already have this skip this step. mydata$treated = ifelse(mydata$country == &quot;E&quot; | mydata$country == &quot;F&quot; | mydata$country == &quot;G&quot;, 1, 0) 3.did: an interaction by multiplying time and treated. We will call this interaction ‘did’. mydata$did = mydata$time * mydata$treated 7.2 The DID estimator didreg = lm(y ~ treated + time + did, data = mydata) summary(didreg) ## ## Call: ## lm(formula = y ~ treated + time + did, data = mydata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.768e+09 -1.623e+09 1.167e+08 1.393e+09 6.807e+09 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.581e+08 7.382e+08 0.485 0.6292 ## treated 1.776e+09 1.128e+09 1.575 0.1200 ## time 2.289e+09 9.530e+08 2.402 0.0191 * ## did -2.520e+09 1.456e+09 -1.731 0.0882 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.953e+09 on 66 degrees of freedom ## Multiple R-squared: 0.08273, Adjusted R-squared: 0.04104 ## F-statistic: 1.984 on 3 and 66 DF, p-value: 0.1249 The coefficient for ‘did’ is the differences-in-differences estimator. The effect is significant at 10% with the treatment having a negative effect. 7.3 Card and Krueger (1994) from PoEdata The “PoEdata”” package loads into R the data sets that accompany Principles of Econometrics 4e, by Carter Hill, William Griffiths, and Guay Lim. by Dr. Constantin Colonescu https://github.com/ccolonescu/PoEdata https://bookdown.org/ccolonescu/RPoE4/indvars.html#the-difference-in-differences-estimator data(&quot;njmin3&quot;, package=&quot;PoEdata&quot;) ?njmin3 # minimum wage example 7.3.1 Regression models mod1 &lt;- lm(fte~nj*d, data=njmin3) mod2 &lt;- lm(fte~nj*d+ kfc+roys+wendys+co_owned, data=njmin3) mod3 &lt;- lm(fte~nj*d+ kfc+roys+wendys+co_owned+ southj+centralj+pa1, data=njmin3) stargazer::stargazer(mod1, mod2, mod3, type = &#39;text&#39;, model.names = FALSE, header=FALSE, keep.stat=&quot;n&quot;,digits=2, column.labels = c(&#39;DID&#39;, &#39;DIDw/Cov&#39;, &#39;DIDw/All&#39;)) ## ## ========================================== ## Dependent variable: ## ----------------------------- ## fte ## DID DIDw/Cov DIDw/All ## (1) (2) (3) ## ------------------------------------------ ## nj -2.89** -2.38** -0.91 ## (1.19) (1.08) (1.27) ## ## d -2.17 -2.22 -2.21 ## (1.52) (1.37) (1.35) ## ## kfc -10.45*** -10.06*** ## (0.85) (0.84) ## ## roys -1.62* -1.69** ## (0.86) (0.86) ## ## wendys -1.06 -1.06 ## (0.93) (0.92) ## ## co_owned -1.17 -0.72 ## (0.72) (0.72) ## ## southj -3.70*** ## (0.78) ## ## centralj 0.01 ## (0.90) ## ## pa1 0.92 ## (1.38) ## ## nj:d 2.75 2.85* 2.81* ## (1.69) (1.52) (1.50) ## ## Constant 23.33*** 25.95*** 25.32*** ## (1.07) (1.04) (1.21) ## ## ------------------------------------------ ## Observations 794 794 794 ## ========================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 7.4 Demo of HonestDiD (Rambachan &amp; Roth, 2022) See original tutorial: https://github.com/asheshrambachan/HonestDiD library(here) library(dplyr) library(did) library(Rglpk) library(haven) library(ggplot2) library(fixest) library(HonestDiD) df &lt;- read_dta(&quot;https://raw.githubusercontent.com/Mixtape-Sessions/Advanced-DID/main/Exercises/Data/ehec_data.dta&quot;) head(df,5) ## # A tibble: 5 × 5 ## stfips year dins yexp2 W ## &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 [alabama] 2008 [2008] 0.681 NA 613156 ## 2 1 [alabama] 2009 [2009] 0.658 NA 613156 ## 3 1 [alabama] 2010 [2010] 0.631 NA 613156 ## 4 1 [alabama] 2011 [2011] 0.656 NA 613156 ## 5 1 [alabama] 2012 [2012] 0.671 NA 613156 7.4.1 Estimate the baseline DiD #Keep years before 2016. Drop the 2016 cohort df_nonstaggered &lt;- df %&gt;% filter(year &lt; 2016 &amp; (is.na(yexp2)| yexp2 != 2015) ) #Create a treatment dummy df_nonstaggered &lt;- df_nonstaggered %&gt;% mutate(D = case_when( yexp2 == 2014 ~ 1, T ~ 0)) #Run the TWFE spec twfe_results &lt;- fixest::feols(dins ~ i(year, D, ref = 2013) | stfips + year, cluster = &quot;stfips&quot;, data = df_nonstaggered) betahat &lt;- summary(twfe_results)$coefficients #save the coefficients sigma &lt;- summary(twfe_results)$cov.scaled #save the covariance matrix fixest::iplot(twfe_results) 7.4.2 Sensitivity analysis using relative magnitudes restrictions delta_rm_results &lt;- HonestDiD::createSensitivityResults_relativeMagnitudes( betahat = betahat, #coefficients sigma = sigma, #covariance matrix numPrePeriods = 5, #num. of pre-treatment coefs numPostPeriods = 2, #num. of post-treatment coefs Mbarvec = seq(0.5,2,by=0.5) #values of Mbar ) delta_rm_results ## # A tibble: 4 × 5 ## lb ub method Delta Mbar ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.0240 0.0672 C-LF DeltaRM 0.5 ## 2 0.0170 0.0720 C-LF DeltaRM 1 ## 3 0.00824 0.0797 C-LF DeltaRM 1.5 ## 4 -0.000916 0.0881 C-LF DeltaRM 2 originalResults &lt;- HonestDiD::constructOriginalCS(betahat = betahat, sigma = sigma, numPrePeriods = 5, numPostPeriods = 2) HonestDiD::createSensitivityPlot_relativeMagnitudes(delta_rm_results, originalResults) 7.5 Synthetic Control library(devtools) devtools::install_github(&quot;edunford/tidysynth&quot;) library(tidysynth) 7.5.1 Smoking data See original tutorial: https://github.com/edunford/tidysynth data(&quot;smoking&quot;) smoking %&gt;% dplyr::glimpse() ## Rows: 1,209 ## Columns: 7 ## $ state &lt;chr&gt; &quot;Rhode Island&quot;, &quot;Tennessee&quot;, &quot;Indiana&quot;, &quot;Nevada&quot;, &quot;Louisiana&quot;, &quot;Okl… ## $ year &lt;dbl&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1… ## $ cigsale &lt;dbl&gt; 123.9, 99.8, 134.6, 189.5, 115.9, 108.4, 265.7, 93.8, 100.3, 124.3,… ## $ lnincome &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ beer &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ age15to24 &lt;dbl&gt; 0.1831579, 0.1780438, 0.1765159, 0.1615542, 0.1851852, 0.1754592, 0… ## $ retprice &lt;dbl&gt; 39.3, 39.9, 30.6, 38.9, 34.3, 38.4, 31.4, 37.3, 36.7, 28.8, 41.4, 3… 7.5.2 Generate Control The method aims to generate a synthetic California using information from a subset of control states (the “donor pool”) where a similar law was not implemented. The donor pool is the subset of case comparisons from which information is borrowed to generate a synthetic version of the treated unit (“California”). smoking_out &lt;- smoking %&gt;% # initial the synthetic control object synthetic_control(outcome = cigsale, # outcome unit = state, # unit index in the panel data time = year, # time index in the panel data i_unit = &quot;California&quot;, # unit where the intervention occurred i_time = 1988, # time period when the intervention occurred generate_placebos=T # generate placebo synthetic controls (for inference) ) %&gt;% # Generate the aggregate predictors used to fit the weights # average log income, retail price of cigarettes, and proportion of the # population between 15 and 24 years of age from 1980 - 1988 generate_predictor(time_window = 1980:1988, ln_income = mean(lnincome, na.rm = T), ret_price = mean(retprice, na.rm = T), youth = mean(age15to24, na.rm = T)) %&gt;% # average beer consumption in the donor pool from 1984 - 1988 generate_predictor(time_window = 1984:1988, beer_sales = mean(beer, na.rm = T)) %&gt;% # Lagged cigarette sales generate_predictor(time_window = 1975, cigsale_1975 = cigsale) %&gt;% generate_predictor(time_window = 1980, cigsale_1980 = cigsale) %&gt;% generate_predictor(time_window = 1988, cigsale_1988 = cigsale) %&gt;% # Generate the fitted weights for the synthetic control generate_weights(optimization_window = 1970:1988, # time to use in the optimization task margin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 # optimizer options ) %&gt;% # Generate the synthetic control generate_control() Once the synthetic control is generated, one can easily assess the fit by comparing the trends of the synthetic and observed time series. The idea is that the trends in the pre-intervention period should map closely onto one another. smoking_out %&gt;% plot_trends() To capture the causal quantity (i.e. the difference between the observed and counterfactual), one can plot the differences using plot_differences() smoking_out %&gt;% plot_differences() In addition, one can easily examine the weighting of the units and variables in the fit. This allows one to see which cases were used, in part, to generate the synthetic control. smoking_out %&gt;% plot_weights() Another useful way of evaluating the synthetic control is to look at how comparable the synthetic control is to the observed covariates of the treated unit. smoking_out %&gt;% grab_balance_table() ## # A tibble: 7 × 4 ## variable California synthetic_California donor_sample ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ln_income 10.1 9.85 9.83 ## 2 ret_price 89.4 89.4 87.3 ## 3 youth 0.174 0.174 0.173 ## 4 beer_sales 24.3 24.2 23.7 ## 5 cigsale_1975 127. 127. 137. ## 6 cigsale_1980 120. 120. 138. ## 7 cigsale_1988 90.1 91.4 114. 7.5.3 Inference For inference, the method relies on repeating the method for every donor in the donor pool exactly as was done for the treated unit - i.e. generating placebo synthetic controls). By setting generate_placebos = TRUE when initializing the synth pipeline with synthetic_control(), placebo cases are automatically generated when constructing the synthetic control of interest. This makes it easy to explore how unique difference between the observed and synthetic unit is when compared to the placebos. smoking_out %&gt;% plot_placebos() there is a significance table that can be extracted with one of the many grab_ prefix functions. smoking_out %&gt;% grab_significance() ## # A tibble: 39 × 8 ## unit_name type pre_mspe post_mspe mspe_ratio rank fishers_exact_p…¹ z_score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 California Treated 3.17 392. 124. 1 0.0256 5.32 ## 2 Georgia Donor 3.79 179. 47.2 2 0.0513 1.70 ## 3 Indiana Donor 25.2 770. 30.6 3 0.0769 0.916 ## 4 West Virginia Donor 9.52 284. 29.8 4 0.103 0.881 ## 5 Wisconsin Donor 11.1 268. 24.1 5 0.128 0.607 ## 6 Missouri Donor 3.03 67.8 22.4 6 0.154 0.528 ## 7 Texas Donor 14.4 277. 19.3 7 0.179 0.383 ## 8 South Carolina Donor 12.6 234. 18.6 8 0.205 0.351 ## 9 Virginia Donor 9.81 96.4 9.83 9 0.231 -0.0646 ## 10 Nebraska Donor 6.30 52.9 8.40 10 0.256 -0.132 ## # … with 29 more rows, and abbreviated variable name ¹​fishers_exact_pvalue "]]
