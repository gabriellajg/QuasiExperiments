[["index.html", "R Book for Quasi-Experimental Designs Chapter 1 Course", " R Book for Quasi-Experimental Designs Ge Jiang QUERIES, University of Illinois at Urbana-Champaign Chapter 1 Course Welcome to Quasi-Experimental Designs! This course focuses on the analysis of some of the strongest quasi-experimental designs such as regression discontinuity, interrupted time series, propensity score matching, and instrumental variable methods. Social scientists have become increasingly interested in the causal effects of specific policies or practices. For example, an education researcher may want to know if curriculum A produces higher reading achievement scores than curriculum B does for third-graders. And if there is a difference in achievement scores between the two groups of students, what is the magnitude of the effect? Experiments (e.g., randomized control trials) are often referred to as the “gold standard” for determining the effect of a policy or practice (relative to some other policy or practice) on a population of interest. However, experimental designs can in some cases be time-consuming, costly, unethical, or otherwise impractical. Given these considerations and the wealth of already existing observational data, researchers have crafted careful approximations to randomized control trials that utilize the already existing data to learn more about social science phenomena. This class of research designs is referred to as “quasi-experimental designs.” Causal research questions like “Did No Child Left Behind (NCLB) increase students’ achievement in reading and math?” or “Does retaining kindergartners for one year (instead of promoting them) result in negative effects on their future achievements?” are typically investigated using quasi-experimental designs. Specifically, we will learn about the assumptions, theories, and application of each of the prominent quasi-experimental methods. This site is supposed to serve as a repository for R codes used in lab sessions of a graduate-level method course EPSY 574. *Disclaimer: Opinions are my own and not the views of my employer. "],["regression-approach-to-treatment-effect-estimation.html", "Chapter 2 Regression Approach to Treatment Effect Estimation 2.1 Regression w/ no confounder 2.2 Including confounder SES", " Chapter 2 Regression Approach to Treatment Effect Estimation Suppose one would like to use a regression model to estimate the treatment effect of a SAT, but controlling for the covariate ‘SES’. Simulating data: ID = c(1:6) Grp = rep(c(0, 1), each = 3) Score = c(550, 600, 650, 600, 720, 630) SES = c(1, 2, 2, 2, 3, 2) SATdat = data.frame(ID, Grp, Score, SES) SATdat$SES = factor(SATdat$SES) 2.1 Regression w/ no confounder lm1 = lm(Score ~ Grp, data = SATdat) summary(lm1) ## ## Call: ## lm(formula = Score ~ Grp, data = SATdat) ## ## Residuals: ## 1 2 3 4 5 6 ## -5.000e+01 -7.638e-14 5.000e+01 -5.000e+01 7.000e+01 -2.000e+01 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 600.00 32.66 18.371 5.17e-05 *** ## Grp 50.00 46.19 1.083 0.34 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56.57 on 4 degrees of freedom ## Multiple R-squared: 0.2266, Adjusted R-squared: 0.03323 ## F-statistic: 1.172 on 1 and 4 DF, p-value: 0.3399 This is the same as a simple t-test: t.test(SATdat$Score[SATdat$Grp==1], SATdat$Score[SATdat$Grp==0]) ## ## Welch Two Sample t-test ## ## data: SATdat$Score[SATdat$Grp == 1] and SATdat$Score[SATdat$Grp == 0] ## t = 1.0825, df = 3.8173, p-value = 0.3426 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -80.69522 180.69522 ## sample estimates: ## mean of x mean of y ## 650 600 not sig – sample size is too small 2.2 Including confounder SES ses.lm = lm(Score ~ Grp+SES, data = SATdat) summary(ses.lm) ## ## Call: ## lm(formula = Score ~ Grp + SES, data = SATdat) ## ## Residuals: ## 1 2 3 4 5 6 ## 1.066e-14 -2.500e+01 2.500e+01 -1.500e+01 -4.016e-15 1.500e+01 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 550.00 29.15 18.865 0.0028 ** ## Grp -10.00 29.15 -0.343 0.7643 ## SES2 75.00 35.71 2.100 0.1705 ## SES3 180.00 50.50 3.565 0.0705 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.15 on 2 degrees of freedom ## Multiple R-squared: 0.8973, Adjusted R-squared: 0.7432 ## F-statistic: 5.824 on 3 and 2 DF, p-value: 0.1501 The effect (-10) is assumed between groups WITHIN EACH SES LEVEL: 2.2.1 low SES predict(ses.lm, newdata = data.frame(Grp=0, SES=factor(1))) ## 1 ## 550 predict(ses.lm, newdata = data.frame(Grp=1, SES=factor(1))) ## 1 ## 540 2.2.2 middle SES predict(ses.lm, newdata = data.frame(Grp=0, SES=factor(2))) ## 1 ## 625 predict(ses.lm, newdata = data.frame(Grp=1, SES=factor(2))) ## 1 ## 615 2.2.3 high SES predict(ses.lm, newdata = data.frame(Grp=0, SES=factor(3))) ## 1 ## 730 predict(ses.lm, newdata = data.frame(Grp=1, SES=factor(3))) ## 1 ## 720 "],["types-of-causal-effects.html", "Chapter 3 Types of Causal Effects 3.1 ATT, ATE, ATU, … what the what? 3.2 Calculating different causal effects in practice", " Chapter 3 Types of Causal Effects Sometimes our experiments because quasi-experiments because of crossover or non-compliance. Simulating data on assignment, compliance, and potential outcomes: set.seed(1) assign = rbinom(100, 1, 0.5) # half half Simulating non-compliance as a function of gender: set.seed(101) gender = rbinom(100, 1, 0.4) # 40% are men comply = rep(); comply[gender==1] = rbinom(sum(gender==1), 1, 0.5) # prob of men complying is 0.5 comply[gender==0] = rbinom(sum(gender==0), 1, 0.7) # prob of women complying is 0.7 treatment effect: 100 (from 400 –&gt; 500) fixed for everyone: notutor = rnorm(100, 400, 12) # Y0 tutor = notutor + 100 + rnorm(100, 0, 20) # Y1 simdata = data.frame(assign, notutor, tutor) head(simdata) ## assign notutor tutor ## 1 0 403.2168 499.9361 ## 2 0 392.8935 465.2280 ## 3 1 425.6018 534.0721 ## 4 1 414.0730 498.2632 ## 5 0 408.9611 533.1596 ## 6 1 397.2339 515.1242 actual treatment: treat = ifelse(comply==1, assign, 1-assign) table(assign, treat) ## treat ## assign 0 1 ## 0 31 21 ## 1 22 26 observed value: obs = treat*tutor + (1-treat)*notutor # Y1 if T=1 and Y0 if T=0 when treat=1: Y = 1tutor + 0notutor = Y1 when treat=0: Y = 0tutor + 1notutor = Y0 Simulated data: simdata = data.frame(assign, comply, treat, tutor, notutor, obs, gender) head(simdata) ## assign comply treat tutor notutor obs gender ## 1 0 1 0 499.9361 403.2168 403.2168 0 ## 2 0 1 0 465.2280 392.8935 392.8935 0 ## 3 1 0 0 534.0721 425.6018 425.6018 1 ## 4 1 0 0 498.2632 414.0730 414.0730 1 ## 5 0 1 0 533.1596 408.9611 408.9611 0 ## 6 1 1 1 515.1242 397.2339 515.1242 0 3.1 ATT, ATE, ATU, … what the what? ATE/ACE: Average Treatment Effect in the population ATT: Average Treatment Effect on the Treated ATU: Average Treatment Effect on the Untreated LATE: Local Average Treatment Effect for Compilers CATE: Conditional Average Treatment Effect ITT: Intention To Treat. Effect of simply the intention (instead of receipt) of treatment ATE = mean(simdata$tutor - simdata$notutor) # whole population ATT = mean((simdata$tutor - simdata$notutor)[treat==1]) ATU = mean((simdata$tutor - simdata$notutor)[treat==0]) LATE = mean((simdata$tutor - simdata$notutor)[comply==1]) CATE0 = mean((simdata$tutor - simdata$notutor)[gender==0]) CATE1 = mean((simdata$tutor - simdata$notutor)[gender==1]) cbind(ATE, ATT, ATU, LATE, CATE0, CATE1) ## ATE ATT ATU LATE CATE0 CATE1 ## [1,] 100.0414 101.1523 99.05629 99.88937 102.4829 97.28824 # all close to ATE 3.2 Calculating different causal effects in practice ATE ATE_obs = lm(obs ~ treat, data = simdata) coef(ATE_obs) # 99.82048 ## (Intercept) treat ## 400.12192 99.82048 ATT &amp; ATU no way to estimate ATT &amp; ATU from data LATE if compliance status is known LATE_obs = lm(obs ~ treat, data = simdata[simdata$comply==1,]) coef(LATE_obs) # 101.1594 ## (Intercept) treat ## 398.5844 101.1594 CATE if gender is included as a confounder: CATE0_obs = lm(obs ~ treat, data = simdata[simdata$gender==0,]) coef(CATE0_obs) # 106.5414 ## (Intercept) treat ## 400.1959 106.5414 CATE1_obs = lm(obs ~ treat, data = simdata[simdata$gender==1,]) coef(CATE1_obs) # 90.71861 ## (Intercept) treat ## 400.05067 90.71861 Intention To Treat (ITT) ITT = lm(obs ~ assign) coef(ITT) # 15.460 whereas the true effect is 100!! ## (Intercept) assign ## 439.61675 15.45999 Intention To Treat on Compliers ITT_c = lm(obs ~ assign, data = simdata[simdata$comply==1,]) coef(ITT_c) # 101.159 which is pretty close to the true effect 100 ## (Intercept) assign ## 398.5844 101.1594 Generally recommended: LATE or ITT on Compliers! "],["instrumental-variable.html", "Chapter 4 Instrumental Variable 4.1 Loading Data 4.2 Setup 4.3 Regression Approaches 4.4 IV: Does education effect wages when college proximity is used as the instrument? 4.5 Take-home exercise", " Chapter 4 Instrumental Variable In this lab, we will use Card (1995) to see how we can use an instrumental variable (distance to the nearest college) to estimate the effect of schooling on wage. 4.1 Loading Data Load Stata file: library(psych) library(foreign) library(car) card.data&lt;-read.dta(&quot;card.dta&quot;) See the description of the data here: https://www.ssc.wisc.edu/~bhansen/econometrics/Card1995_description.pdf Preview the data: attach(card.data) ## The following objects are masked from bwght (pos = 3): ## ## fatheduc, motheduc ## The following objects are masked from card.data (pos = 4): ## ## age, black, educ, enroll, exper, expersq, fatheduc, id, iq, kww, libcrd14, lwage, ## married, momdad14, motheduc, nearc2, nearc4, reg661, reg662, reg663, reg664, ## reg665, reg666, reg667, reg668, reg669, sinmom14, smsa, smsa66, south, south66, ## step14, wage, weight ## The following objects are masked from bwght (pos = 26): ## ## fatheduc, motheduc ## The following objects are masked from card.data (pos = 27): ## ## age, black, educ, enroll, exper, expersq, fatheduc, id, iq, kww, libcrd14, lwage, ## married, momdad14, motheduc, nearc2, nearc4, reg661, reg662, reg663, reg664, ## reg665, reg666, reg667, reg668, reg669, sinmom14, smsa, smsa66, south, south66, ## step14, wage, weight ## The following objects are masked from bwght (pos = 40): ## ## fatheduc, motheduc ## The following objects are masked from card.data (pos = 47): ## ## age, black, educ, enroll, exper, expersq, fatheduc, id, iq, kww, libcrd14, lwage, ## married, momdad14, motheduc, nearc2, nearc4, reg661, reg662, reg663, reg664, ## reg665, reg666, reg667, reg668, reg669, sinmom14, smsa, smsa66, south, south66, ## step14, wage, weight head(card.data) ## id nearc2 nearc4 educ age fatheduc motheduc weight momdad14 sinmom14 step14 reg661 reg662 ## 1 2 0 0 7 29 NA NA 158413 1 0 0 1 0 ## 2 3 0 0 12 27 8 8 380166 1 0 0 1 0 ## 3 4 0 0 12 34 14 12 367470 1 0 0 1 0 ## 4 5 1 1 11 27 11 12 380166 1 0 0 0 1 ## 5 6 1 1 12 34 8 7 367470 1 0 0 0 1 ## 6 7 1 1 12 26 9 12 380166 1 0 0 0 1 ## reg663 reg664 reg665 reg666 reg667 reg668 reg669 south66 black smsa south smsa66 wage enroll ## 1 0 0 0 0 0 0 0 0 1 1 0 1 548 0 ## 2 0 0 0 0 0 0 0 0 0 1 0 1 481 0 ## 3 0 0 0 0 0 0 0 0 0 1 0 1 721 0 ## 4 0 0 0 0 0 0 0 0 0 1 0 1 250 0 ## 5 0 0 0 0 0 0 0 0 0 1 0 1 729 0 ## 6 0 0 0 0 0 0 0 0 0 1 0 1 500 0 ## kww iq married libcrd14 exper lwage expersq ## 1 15 NA 1 0 16 6.306275 256 ## 2 35 93 1 1 9 6.175867 81 ## 3 42 103 1 1 16 6.580639 256 ## 4 25 88 1 1 10 5.521461 100 ## 5 34 108 1 0 16 6.591674 256 ## 6 38 85 1 1 8 6.214608 64 4.2 Setup Treatment: educ (years of education) Outcome: lwage (log_wage) IV: nearc4 (whether live close to a four-year college or not) pairs.panels(card.data[,c(&#39;lwage&#39;,&#39;educ&#39;, &#39;nearc4&#39;)]) 4.3 Regression Approaches 4.3.1 Naive Regression: OLS estimate with treatment only m0&lt;-lm(lwage~educ, data = card.data) summary(m0) ## ## Call: ## lm(formula = lwage ~ educ, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.73799 -0.27764 0.02373 0.28839 1.46080 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.57088 0.03883 143.47 &lt;2e-16 *** ## educ 0.05209 0.00287 18.15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4214 on 3008 degrees of freedom ## Multiple R-squared: 0.09874, Adjusted R-squared: 0.09844 ## F-statistic: 329.5 on 1 and 3008 DF, p-value: &lt; 2.2e-16 4.3.2 Regression with covariates We find education is SSD, but we can make the case that it is endogenous. m1&lt;-lm(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66, data = card.data) summary(m1) ## ## Call: ## lm(formula = lwage ~ educ + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.62326 -0.22141 0.02001 0.23932 1.33340 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.7393766 0.0715282 66.259 &lt; 2e-16 *** ## educ 0.0746933 0.0034983 21.351 &lt; 2e-16 *** ## exper 0.0848320 0.0066242 12.806 &lt; 2e-16 *** ## expersq -0.0022870 0.0003166 -7.223 6.41e-13 *** ## black -0.1990123 0.0182483 -10.906 &lt; 2e-16 *** ## south -0.1479550 0.0259799 -5.695 1.35e-08 *** ## smsa 0.1363845 0.0201005 6.785 1.39e-11 *** ## reg661 -0.1185698 0.0388301 -3.054 0.002281 ** ## reg662 -0.0222026 0.0282575 -0.786 0.432092 ## reg663 0.0259703 0.0273644 0.949 0.342670 ## reg664 -0.0634942 0.0356803 -1.780 0.075254 . ## reg665 0.0094551 0.0361174 0.262 0.793503 ## reg666 0.0219476 0.0400984 0.547 0.584182 ## reg667 -0.0005887 0.0393793 -0.015 0.988073 ## reg668 -0.1750058 0.0463394 -3.777 0.000162 *** ## smsa66 0.0262417 0.0194477 1.349 0.177327 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3723 on 2994 degrees of freedom ## Multiple R-squared: 0.2998, Adjusted R-squared: 0.2963 ## F-statistic: 85.48 on 15 and 2994 DF, p-value: &lt; 2.2e-16 4.4 IV: Does education effect wages when college proximity is used as the instrument? Is college proximity an exogenous determinant of wages? 4.4.1 Stage 1: T on IV m2&lt;-lm(educ~nearc4+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66, data = card.data) summary(m2) ## ## Call: ## lm(formula = educ ~ nearc4 + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.545 -1.370 -0.091 1.278 6.239 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.8485239 0.2111222 79.805 &lt; 2e-16 *** ## nearc4 0.3198989 0.0878638 3.641 0.000276 *** ## exper -0.4125334 0.0336996 -12.241 &lt; 2e-16 *** ## expersq 0.0008686 0.0016504 0.526 0.598728 ## black -0.9355287 0.0937348 -9.981 &lt; 2e-16 *** ## south -0.0516126 0.1354284 -0.381 0.703152 ## smsa 0.4021825 0.1048112 3.837 0.000127 *** ## reg661 -0.2102710 0.2024568 -1.039 0.299076 ## reg662 -0.2889073 0.1473395 -1.961 0.049992 * ## reg663 -0.2382099 0.1426357 -1.670 0.095012 . ## reg664 -0.0930890 0.1859827 -0.501 0.616742 ## reg665 -0.4828875 0.1881872 -2.566 0.010336 * ## reg666 -0.5130857 0.2096352 -2.448 0.014442 * ## reg667 -0.4270887 0.2056208 -2.077 0.037880 * ## reg668 0.3136204 0.2416739 1.298 0.194490 ## smsa66 0.0254805 0.1057692 0.241 0.809644 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.941 on 2994 degrees of freedom ## Multiple R-squared: 0.4771, Adjusted R-squared: 0.4745 ## F-statistic: 182.1 on 15 and 2994 DF, p-value: &lt; 2.2e-16 Predicted part in T: educ_hat = fitted(m2) 4.4.1.1 test of weak instrument - rejected linearHypothesis(m2, c(&quot;nearc4=0&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## nearc4 = 0 ## ## Model 1: restricted model ## Model 2: educ ~ nearc4 + exper + expersq + black + south + smsa + reg661 + ## reg662 + reg663 + reg664 + reg665 + reg666 + reg667 + reg668 + ## smsa66 ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 2995 11324 ## 2 2994 11274 1 49.917 13.256 0.0002763 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.4.2 Stage 2: Y on T_hat m2b = lm(lwage~ educ_hat+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66, data = card.data) summary(m2b) ## ## Call: ## lm(formula = lwage ~ educ_hat + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66, data = card.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.57387 -0.25161 0.01483 0.27229 1.38522 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7739651 0.9612564 3.926 8.83e-05 *** ## educ_hat 0.1315038 0.0565103 2.327 0.020028 * ## exper 0.1082711 0.0243243 4.451 8.85e-06 *** ## expersq -0.0023349 0.0003429 -6.810 1.18e-11 *** ## black -0.1467757 0.0554166 -2.649 0.008125 ** ## south -0.1446715 0.0280524 -5.157 2.67e-07 *** ## smsa 0.1118083 0.0325530 3.435 0.000601 *** ## reg661 -0.1078142 0.0429903 -2.508 0.012199 * ## reg662 -0.0070465 0.0338333 -0.208 0.835032 ## reg663 0.0404445 0.0326749 1.238 0.215892 ## reg664 -0.0579172 0.0386641 -1.498 0.134250 ## reg665 0.0384577 0.0482596 0.797 0.425577 ## reg666 0.0550887 0.0541416 1.017 0.309001 ## reg667 0.0267580 0.0502027 0.533 0.594074 ## reg668 -0.1908912 0.0521383 -3.661 0.000255 *** ## smsa66 0.0185311 0.0222167 0.834 0.404286 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3993 on 2994 degrees of freedom ## Multiple R-squared: 0.1947, Adjusted R-squared: 0.1907 ## F-statistic: 48.25 on 15 and 2994 DF, p-value: &lt; 2.2e-16 4.4.3 All-in-one function ivreg(): library(AER) m4&lt;-ivreg(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66 | nearc4+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66) formula, instruments formula specification(s) of the regression relationship and the instruments. instruments is missing and formula has three parts as in y ~ x1 + x2 | z1 + z2 + z3 (recommended) Alternatively: m4&lt;-ivreg(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+ reg664+reg665+reg666+reg667+reg668+smsa66 | .-educ+nearc4) to the left of |: outcome = lwage to the left of |: variables = educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+reg664+reg665+reg666+reg667+reg668+smsa66 to the right of |: variables = nearc4+exper+expersq+black+south+smsa+reg661+reg662+reg663+reg664+reg665+reg666+reg667+reg668+smsa66 to the right of |: outcome = educ to the right of |: 1st stage of 2SLS to the left of |: 2nd stage of 2SLS ‘.’ means all variables in the dataset besides the one to the left of lwage treatment variable instrument Test using sandwich standard errors: summary(m4, vcov = sandwich, diagnostics = TRUE) ## ## Call: ## ivreg(formula = lwage ~ educ + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66 | . - educ + nearc4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.83164 -0.24075 0.02428 0.25208 1.42760 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7739651 0.9174053 4.114 4.00e-05 *** ## educ 0.1315038 0.0539995 2.435 0.014938 * ## exper 0.1082711 0.0233466 4.638 3.68e-06 *** ## expersq -0.0023349 0.0003478 -6.713 2.27e-11 *** ## black -0.1467757 0.0523622 -2.803 0.005094 ** ## south -0.1446715 0.0290653 -4.977 6.81e-07 *** ## smsa 0.1118083 0.0310619 3.600 0.000324 *** ## reg661 -0.1078142 0.0409668 -2.632 0.008538 ** ## reg662 -0.0070465 0.0336994 -0.209 0.834387 ## reg663 0.0404445 0.0325208 1.244 0.213725 ## reg664 -0.0579172 0.0392106 -1.477 0.139759 ## reg665 0.0384577 0.0494675 0.777 0.436965 ## reg666 0.0550887 0.0521309 1.057 0.290716 ## reg667 0.0267580 0.0501066 0.534 0.593367 ## reg668 -0.1908912 0.0506897 -3.766 0.000169 *** ## smsa66 0.0185311 0.0205103 0.904 0.366333 ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 1 2994 14.214 0.000166 *** ## Wu-Hausman 1 2993 1.219 0.269649 ## Sargan 0 NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3883 on 2994 degrees of freedom ## Multiple R-Squared: 0.2382, Adjusted R-squared: 0.2343 ## Wald test: 56.06 on 15 and 2994 DF, p-value: &lt; 2.2e-16 4.4.4 Diagnostic tests: df1 df2 statistic p-value Weak instruments 1 2994 14.214 0.000166 *** Wu-Hausman 1 2993 1.219 0.269649 Sargan 0 NA NA NA Weak instruments means that the IV has a low correlation with the treatment variable. The null is that the IV is weak. If the null is rejected, so you can move forward with the assumption that the instrument is sufficiently strong. Applied to 2SLS regression, the Wu–Hausman test is a test of endogenity. If all of the regressors are exogenous, then both the OLS and 2SLS estimators are consistent, and the OLS estimator is more efficient, but if one or more regressors are endogenous, then the OLS estimator is inconsistent. A large test statistic and small p-value suggests that the OLS estimator is inconsistent and the 2SLS estimator is therefore to be preferred. 4.4.5 Multiple IV #m5&lt;-ivreg(lwage~educ | .-educ+nearc4+nearc2) m5&lt;-ivreg(lwage~educ+exper+expersq+black+south+smsa+reg661+reg662+reg663+reg664+reg665+reg666+reg667+reg668+smsa66 | .-educ+nearc4+nearc2) summary(m5, vcov = sandwich, diagnostics = TRUE) ## ## Call: ## ivreg(formula = lwage ~ educ + exper + expersq + black + south + ## smsa + reg661 + reg662 + reg663 + reg664 + reg665 + reg666 + ## reg667 + reg668 + smsa66 | . - educ + nearc4 + nearc2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.93841 -0.25068 0.01932 0.26519 1.46998 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.3396868 0.8909170 3.749 0.000181 *** ## educ 0.1570594 0.0524127 2.997 0.002753 ** ## exper 0.1188149 0.0228905 5.191 2.24e-07 *** ## expersq -0.0023565 0.0003674 -6.414 1.64e-10 *** ## black -0.1232778 0.0514904 -2.394 0.016718 * ## south -0.1431945 0.0301873 -4.744 2.20e-06 *** ## smsa 0.1007530 0.0313621 3.213 0.001329 ** ## reg661 -0.1029760 0.0425755 -2.419 0.015637 * ## reg662 -0.0002286 0.0345230 -0.007 0.994716 ## reg663 0.0469556 0.0335252 1.401 0.161435 ## reg664 -0.0554084 0.0408927 -1.355 0.175529 ## reg665 0.0515041 0.0506274 1.017 0.309085 ## reg666 0.0699968 0.0534531 1.309 0.190466 ## reg667 0.0390596 0.0514309 0.759 0.447639 ## reg668 -0.1980371 0.0522335 -3.791 0.000153 *** ## smsa66 0.0150626 0.0211120 0.713 0.475616 ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 2 2993 8.366 0.000238 *** ## Wu-Hausman 1 2993 2.978 0.084509 . ## Sargan 1 NA 1.248 0.263905 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4053 on 2994 degrees of freedom ## Multiple R-Squared: 0.1702, Adjusted R-squared: 0.166 ## Wald test: 51.65 on 15 and 2994 DF, p-value: &lt; 2.2e-16 Sargan tests overidentification restrictions. The idea is that if you have more than one instrument per endogenous variable, the model is overidentified, and you have some excess information. All of the instruments must be valid for the inferences to be correct. So it tests that all exogenous instruments are in fact exogenous, and uncorrelated with the model residuals. If it is significant, it means that you don’t have valid instruments (somewhere in there, as this is a global test). If it is not significant (our case), this isn’t a concern. 4.4.6 Compare with OLS: library(stargazer) stargazer::stargazer(m0, m1, m2b, m4, type = &#39;text&#39;, model.names = FALSE, column.labels = c(&#39;Naive&#39;, &#39;OLS&#39;, &#39;2SLS&#39;, &#39;IV&#39;), column.sep.width = &quot;15pt&quot;, omit.stat = c(&quot;f&quot;, &quot;ser&quot;)) ## ## =================================================== ## Dependent variable: ## -------------------------------------- ## lwage ## Naive OLS 2SLS IV ## (1) (2) (3) (4) ## --------------------------------------------------- ## educ 0.052*** 0.075*** 0.132** ## (0.003) (0.003) (0.055) ## ## educ_hat 0.132** ## (0.057) ## ## exper 0.085*** 0.108*** 0.108*** ## (0.007) (0.024) (0.024) ## ## expersq -0.002*** -0.002*** -0.002*** ## (0.0003) (0.0003) (0.0003) ## ## black -0.199*** -0.147*** -0.147*** ## (0.018) (0.055) (0.054) ## ## south -0.148*** -0.145*** -0.145*** ## (0.026) (0.028) (0.027) ## ## smsa 0.136*** 0.112*** 0.112*** ## (0.020) (0.033) (0.032) ## ## reg661 -0.119*** -0.108** -0.108*** ## (0.039) (0.043) (0.042) ## ## reg662 -0.022 -0.007 -0.007 ## (0.028) (0.034) (0.033) ## ## reg663 0.026 0.040 0.040 ## (0.027) (0.033) (0.032) ## ## reg664 -0.063* -0.058 -0.058 ## (0.036) (0.039) (0.038) ## ## reg665 0.009 0.038 0.038 ## (0.036) (0.048) (0.047) ## ## reg666 0.022 0.055 0.055 ## (0.040) (0.054) (0.053) ## ## reg667 -0.001 0.027 0.027 ## (0.039) (0.050) (0.049) ## ## reg668 -0.175*** -0.191*** -0.191*** ## (0.046) (0.052) (0.051) ## ## smsa66 0.026 0.019 0.019 ## (0.019) (0.022) (0.022) ## ## Constant 5.571*** 4.739*** 3.774*** 3.774*** ## (0.039) (0.072) (0.961) (0.935) ## ## --------------------------------------------------- ## Observations 3,010 3,010 3,010 3,010 ## R2 0.099 0.300 0.195 0.238 ## Adjusted R2 0.098 0.296 0.191 0.234 ## =================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 4.5 Take-home exercise 4.5.1 Does cigarette smoking have an effect on child birth weight (Wooldridge, 2002)? bwght&lt;-read.dta(&quot;bwght.dta&quot;) head(bwght) ## faminc cigtax cigprice bwght fatheduc motheduc parity male white cigs lbwght bwghtlbs packs ## 1 13.5 16.5 122.3 109 12 12 1 1 1 0 4.691348 6.8125 0 ## 2 7.5 16.5 122.3 133 6 12 2 1 0 0 4.890349 8.3125 0 ## 3 0.5 16.5 122.3 129 NA 12 2 0 0 0 4.859812 8.0625 0 ## 4 15.5 16.5 122.3 126 12 12 2 1 0 0 4.836282 7.8750 0 ## 5 27.5 16.5 122.3 134 14 12 2 1 1 0 4.897840 8.3750 0 ## 6 7.5 16.5 122.3 118 12 14 6 1 0 0 4.770685 7.3750 0 ## lfaminc ## 1 2.6026897 ## 2 2.0149031 ## 3 -0.6931472 ## 4 2.7408400 ## 5 3.3141861 ## 6 2.0149031 attach(bwght) 4.5.2 Missing data colSums(apply(bwght, 2, is.na)) ## faminc cigtax cigprice bwght fatheduc motheduc parity male white cigs ## 0 0 0 0 196 1 0 0 0 0 ## lbwght bwghtlbs packs lfaminc ## 0 0 0 0 https://rdrr.io/cran/wooldridge/man/bwght.html A data.frame with 1388 observations on 14 variables: cigtax: cig. tax in home state, 1988 cigprice: cig. price in home state, 1988 (tax already included) faminc: 1988 family income, $1000s lfaminc: log(faminc) fatheduc: father’s yrs of educ motheduc: mother’s yrs of educ parity: birth order of child male: =1 if male child white: =1 if white cigs: cigs smked per day while preg packs: packs smked per day while preg lbwght: log of bwght bwghtlbs: birth weight, pounds bwght: birth weight, ounces library(psych) pairs.panels(bwght[,c(&#39;faminc&#39;,&#39;lfaminc&#39;)]) # lfaminc is better pairs.panels(bwght[,c(&#39;cigs&#39;,&#39;packs&#39;)]) # same, go with cigs pairs.panels(bwght[,c(&#39;lbwght&#39;,&#39;bwghtlbs&#39;, &#39;bwght&#39;)]) # any is good, go with bwghtlbs pairs.panels(bwght[,c(&#39;cigs&#39;,&#39;bwghtlbs&#39;, &#39;cigprice&#39;)]) # low correlations pairs.panels(bwght[,c(&#39;cigs&#39;,&#39;bwghtlbs&#39;, &#39;cigprice&#39;, &#39;lfaminc&#39;, &#39;fatheduc&#39;, &#39;motheduc&#39;, &#39;parity&#39;, &#39;male&#39;, &#39;white&#39;)]) 4.5.3 Your turn now!!!! 4.5.4 Naive Regression: OLS Estimate with treatment only m0&lt;-lm(bwghtlbs ~ cigs, data = bwght) summary(m0) ## ## Call: ## lm(formula = bwghtlbs ~ cigs, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0482 -0.7357 0.0186 0.8268 9.4518 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.485744 0.035771 209.267 &lt; 2e-16 *** ## cigs -0.032111 0.005656 -5.678 1.66e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.258 on 1386 degrees of freedom ## Multiple R-squared: 0.02273, Adjusted R-squared: 0.02202 ## F-statistic: 32.24 on 1 and 1386 DF, p-value: 1.662e-08 4.5.5 Regression with covariates m1&lt;-lm(bwghtlbs ~ cigs+lfaminc+fatheduc+motheduc+parity+male+white, data = bwght) summary(m1) ## ## Call: ## lm(formula = bwghtlbs ~ cigs + lfaminc + fatheduc + motheduc + ## parity + male + white, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8763 -0.7293 0.0241 0.7912 9.5145 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.658642 0.254769 26.136 &lt; 2e-16 *** ## cigs -0.037336 0.006856 -5.446 6.27e-08 *** ## lfaminc 0.076288 0.057027 1.338 0.181234 ## fatheduc 0.025947 0.017417 1.490 0.136565 ## motheduc -0.021047 0.019852 -1.060 0.289270 ## parity 0.119845 0.040955 2.926 0.003496 ** ## male 0.239041 0.071462 3.345 0.000849 *** ## white 0.289841 0.100741 2.877 0.004085 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.228 on 1183 degrees of freedom ## (197 observations deleted due to missingness) ## Multiple R-squared: 0.05444, Adjusted R-squared: 0.04885 ## F-statistic: 9.731 on 7 and 1183 DF, p-value: 7.985e-12 4.5.5.1 test of weak instrument - rejected m2&lt;-lm(cigs ~ cigprice+lfaminc+fatheduc+motheduc+parity+male+white, data = bwght) summary(m2) ## ## Call: ## lm(formula = cigs ~ cigprice + lfaminc + fatheduc + motheduc + ## parity + male + white, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.509 -2.309 -1.466 -0.148 37.113 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.13393 2.09009 3.413 0.000664 *** ## cigprice 0.01366 0.01473 0.928 0.353749 ## lfaminc -0.55819 0.24177 -2.309 0.021129 * ## fatheduc -0.11362 0.07385 -1.538 0.124201 ## motheduc -0.33481 0.08360 -4.005 6.58e-05 *** ## parity 0.13511 0.17357 0.778 0.436485 ## male -0.37175 0.30303 -1.227 0.220151 ## white 0.64796 0.42747 1.516 0.129839 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.204 on 1183 degrees of freedom ## (197 observations deleted due to missingness) ## Multiple R-squared: 0.05705, Adjusted R-squared: 0.05147 ## F-statistic: 10.22 on 7 and 1183 DF, p-value: 1.753e-12 library(car) linearHypothesis(m2, c(&quot;cigprice=0&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## cigprice = 0 ## ## Model 1: restricted model ## Model 2: cigs ~ cigprice + lfaminc + fatheduc + motheduc + parity + male + ## white ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 1184 32066 ## 2 1183 32043 1 23.311 0.8606 0.3537 cigprice, tax included, penalizes the excessive use of cigarettes although cigprice affects cigs, cigprice is not a strong instrument 4.5.6 IV done manually bwght.comp = na.omit(bwght) bwght.comp$cig_hat = fitted(m2) m3 = lm(bwghtlbs ~ cig_hat+lfaminc+fatheduc+motheduc+parity+male+white, data = bwght.comp) summary(m3) ## ## Call: ## lm(formula = bwghtlbs ~ cig_hat + lfaminc + fatheduc + motheduc + ## parity + male + white, data = bwght.comp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8116 -0.7267 0.0585 0.8156 9.5607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.67348 2.28159 2.048 0.04075 * ## cig_hat 0.18797 0.25739 0.730 0.46534 ## lfaminc 0.19868 0.15122 1.314 0.18915 ## fatheduc 0.05079 0.03340 1.521 0.12863 ## motheduc 0.05459 0.08869 0.616 0.53828 ## parity 0.08942 0.05409 1.653 0.09855 . ## male 0.32012 0.11750 2.724 0.00654 ** ## white 0.13817 0.20099 0.687 0.49193 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.243 on 1183 degrees of freedom ## Multiple R-squared: 0.03118, Adjusted R-squared: 0.02544 ## F-statistic: 5.439 on 7 and 1183 DF, p-value: 3.691e-06 same coefficients with ivreg(), but incorrect standard errors 4.5.7 IVreg() library(AER) m4&lt;-ivreg(bwghtlbs ~ cigs+lfaminc+fatheduc+motheduc+parity+male+white | .-cigs+cigprice) summary(m4, vcov = sandwich, diagnostics = TRUE) ## ## Call: ## ivreg(formula = bwghtlbs ~ cigs + lfaminc + fatheduc + motheduc + ## parity + male + white | . - cigs + cigprice) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5996 -0.7003 0.2125 1.1268 9.8161 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.67348 3.03351 1.541 0.1237 ## cigs 0.18797 0.34402 0.546 0.5849 ## lfaminc 0.19868 0.19853 1.001 0.3172 ## fatheduc 0.05079 0.04496 1.130 0.2588 ## motheduc 0.05459 0.11806 0.462 0.6439 ## parity 0.08942 0.07452 1.200 0.2304 ## male 0.32012 0.15398 2.079 0.0378 * ## white 0.13817 0.27141 0.509 0.6108 ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 1 1183 0.914 0.339 ## Wu-Hausman 1 1182 0.825 0.364 ## Sargan 0 NA NA NA ## ## Residual standard error: 1.698 on 1183 degrees of freedom ## Multiple R-Squared: -0.8088, Adjusted R-squared: -0.8195 ## Wald test: 3 on 7 and 1183 DF, p-value: 0.003982 4.5.8 Model Comparison stargazer::stargazer(m0, m1, m3, m4, type = &#39;text&#39;, model.names = FALSE, column.labels = c(&#39;Naive&#39;, &#39;OLS&#39;, &#39;2SLS&#39;, &#39;IV&#39;), column.sep.width = &quot;15pt&quot;, omit.stat = c(&quot;f&quot;, &quot;ser&quot;)) ## ## ================================================= ## Dependent variable: ## ------------------------------------ ## bwghtlbs ## Naive OLS 2SLS IV ## (1) (2) (3) (4) ## ------------------------------------------------- ## cigs -0.032*** -0.037*** 0.188 ## (0.006) (0.007) (0.352) ## ## cig_hat 0.188 ## (0.257) ## ## lfaminc 0.076 0.199 0.199 ## (0.057) (0.151) (0.207) ## ## fatheduc 0.026 0.051 0.051 ## (0.017) (0.033) (0.046) ## ## motheduc -0.021 0.055 0.055 ## (0.020) (0.089) (0.121) ## ## parity 0.120*** 0.089* 0.089 ## (0.041) (0.054) (0.074) ## ## male 0.239*** 0.320*** 0.320** ## (0.071) (0.117) (0.161) ## ## white 0.290*** 0.138 0.138 ## (0.101) (0.201) (0.275) ## ## Constant 7.486*** 6.659*** 4.673** 4.673 ## (0.036) (0.255) (2.282) (3.118) ## ## ------------------------------------------------- ## Observations 1,388 1,191 1,191 1,191 ## R2 0.023 0.054 0.031 -0.809 ## Adjusted R2 0.022 0.049 0.025 -0.819 ## ================================================= ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Since cigprice is NOT a good instrument, we will favor m2 (OLS) over m4 in this case. Negative R2? https://www.stata.com/support/faqs/statistics/two-stage-least-squares/#:~:text=Stata’s%20ivregress%20command%20suppresses%20the,the%20context%20of%202SLS%2FIV. Missing R2s, negative R2s, and negative model sum of squares are all the same issue. However, since our goal is to estimate the structural model, the actual values, not the instruments for the endogenous right-hand-side variables, are used to determine the model sum of squares (MSS). This means a constant-only model of the dependent variable is NOT nested within the two-stage least-squares model, even though the two-stage model estimates an intercept, and the residual sum of squares (RSS) is no longer constrained to be smaller than the total sum of squares (TSS). ybar is a better predictor of y (in the sum-of-squares sense) than Xb! Is that a problem? You can easily develop simulations where the parameter estimates from two-stage are quite good while the MSS is negative. If our two-stage model produces estimates of these parameters with acceptable standard errors, we should be happy—regardless of MSS or R2. "],["psm-matching-strategy.html", "Chapter 5 PSM Matching Strategy 5.1 Data 5.2 Regression Estimates 5.3 PSM Steps", " Chapter 5 PSM Matching Strategy library(Matching) library(MatchIt) library(optmatch) library(weights) library(cem) library(tcltk2) library(knitr) library(CBPS) library(jtools) library(cobalt) library(lmtest) library(sandwich) #vcovCL library(rbounds) #gamma 5.1 Data #data(lalonde) lalonde = MatchIt::lalonde dim(lalonde) ## [1] 614 9 names(lalonde) ## [1] &quot;treat&quot; &quot;age&quot; &quot;educ&quot; &quot;race&quot; &quot;married&quot; &quot;nodegree&quot; &quot;re74&quot; &quot;re75&quot; ## [9] &quot;re78&quot; 5.1.1 Create dummy variable for race and unemployment lalonde$black = ifelse(lalonde$race==&#39;black&#39;, 1, 0) lalonde$hispan = ifelse(lalonde$race==&#39;hispan&#39;, 1, 0) lalonde$un74 = ifelse(lalonde$re74==0, 0, 1) lalonde$un75 = ifelse(lalonde$re75==0, 0, 1) 5.2 Regression Estimates 5.2.1 naive t-test estimate reglm &lt;- lm(re78 ~ treat, data = lalonde) summ(reglm) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(1,612) = 0.93, p = 0.33 ## R² = 0.00 ## Adj. R² = -0.00 ## ## Standard errors: OLS ## ---------------------------------------------------- ## Est. S.E. t val. p ## ----------------- --------- -------- -------- ------ ## (Intercept) 6984.17 360.71 19.36 0.00 ## treat -635.03 657.14 -0.97 0.33 ## ---------------------------------------------------- 5.2.2 regression with covariates reglm1 &lt;- lm(re78 ~ treat + educ + age + black + hispan + married + nodegree + un74 + un75 + re74 + re75, data = lalonde) summ(reglm1) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(11,602) = 10.75, p = 0.00 ## R² = 0.16 ## Adj. R² = 0.15 ## ## Standard errors: OLS ## ------------------------------------------------------ ## Est. S.E. t val. p ## ----------------- ---------- --------- -------- ------ ## (Intercept) 2671.26 2543.96 1.05 0.29 ## treat 865.59 800.01 1.08 0.28 ## educ 349.38 158.47 2.20 0.03 ## age -3.81 32.82 -0.12 0.91 ## black -1466.72 768.90 -1.91 0.06 ## hispan 471.11 934.51 0.50 0.61 ## married 445.25 690.83 0.64 0.52 ## nodegree 3.18 845.33 0.00 1.00 ## un74 -2492.95 807.49 -3.09 0.00 ## un75 -315.32 758.16 -0.42 0.68 ## re74 0.38 0.06 6.05 0.00 ## re75 0.31 0.12 2.69 0.01 ## ------------------------------------------------------ 5.3 PSM Steps 5.3.1 Selection of covariates in X fm1 = treat ~ age + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm2 = treat ~ age + I(age^2) + I(age^3) + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm3 = treat ~ age + I(age^2) + I(age^3) + educ + I(educ^2) + black + hispan + married + I(re74/1000) + I(re75/1000) 5.3.2 Calculation of propensity scores (p-scores) pscore &lt;- glm(fm2, data = lalonde, family = &#39;binomial&#39;) head(pscore$fitted.values) ## NSW1 NSW2 NSW3 NSW4 NSW5 NSW6 ## 0.5871794 0.3014102 0.9056376 0.9020139 0.9069395 0.8230760 hist(pscore$fitted.values[lalonde$treat==0],xlim=c(0,1)) hist(pscore$fitted.values[lalonde$treat==1],xlim=c(0,1)) lalonde$pscore = pscore$fitted.values Logistic regression: summ(pscore) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: treat ## Type: Generalized linear model ## Family: binomial ## Link function: logit ## ## MODEL FIT: ## χ²(9) = 310.83, p = 0.00 ## Pseudo-R² (Cragg-Uhler) = 0.56 ## Pseudo-R² (McFadden) = 0.41 ## AIC = 460.66, BIC = 504.86 ## ## Standard errors: MLE ## -------------------------------------------------- ## Est. S.E. z val. p ## ------------------ -------- ------ -------- ------ ## (Intercept) -18.92 4.22 -4.49 0.00 ## age 1.51 0.44 3.45 0.00 ## I(age^2) -0.04 0.01 -2.73 0.01 ## I(age^3) 0.00 0.00 2.01 0.04 ## educ -0.04 0.05 -0.76 0.45 ## black 3.06 0.30 10.17 0.00 ## hispan 0.68 0.45 1.51 0.13 ## married -1.34 0.31 -4.27 0.00 ## I(re74/1000) -0.10 0.03 -3.11 0.00 ## I(re75/1000) 0.04 0.05 0.82 0.41 ## -------------------------------------------------- try other formulas? 5.3.3 Matching based on p-sores all in one function matchit(): set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm1, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = TRUE, caliper = 0.2, discard = &#39;both&#39; ) summary of matching results: summary(m.out) ## ## Call: ## matchit(formula = fm1, data = lalonde, method = &quot;nearest&quot;, distance = &quot;logit&quot;, ## discard = &quot;both&quot;, replace = TRUE, caliper = 0.2) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.5722 0.1845 1.8020 0.8651 0.3763 0.6428 ## age 25.8162 28.0303 -0.3094 0.4400 0.0813 0.1577 ## educ 10.3459 10.2354 0.0550 0.4959 0.0347 0.1114 ## black 0.8432 0.2028 1.7615 . 0.6404 0.6404 ## hispan 0.0595 0.1422 -0.3498 . 0.0827 0.0827 ## married 0.1892 0.5128 -0.8263 . 0.3236 0.3236 ## I(re74/1000) 2.0956 5.6192 -0.7211 0.5181 0.2248 0.4470 ## I(re75/1000) 1.5321 2.4665 -0.2903 0.9563 0.1342 0.2876 ## ## ## Summary of Balance for Matched Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.5697 0.5692 0.0021 0.9848 0.0031 0.0546 ## age 25.6885 25.5410 0.0206 0.4839 0.0728 0.2404 ## educ 10.3224 10.6667 -0.1712 0.4586 0.0406 0.1475 ## black 0.8415 0.8361 0.0150 . 0.0055 0.0055 ## hispan 0.0601 0.0656 -0.0231 . 0.0055 0.0055 ## married 0.1913 0.1858 0.0140 . 0.0055 0.0055 ## I(re74/1000) 2.1185 2.3747 -0.0524 1.1054 0.0499 0.2896 ## I(re75/1000) 1.5058 1.5431 -0.0116 1.3188 0.0397 0.1967 ## Std. Pair Dist. ## distance 0.0112 ## age 1.0654 ## educ 1.0083 ## black 0.0451 ## hispan 0.2080 ## married 0.3767 ## I(re74/1000) 0.5800 ## I(re75/1000) 0.7106 ## ## Sample Sizes: ## Control Treated ## All 429. 185 ## Matched (ESS) 42.44 183 ## Matched 77. 183 ## Unmatched 278. 0 ## Discarded 74. 2 m.out$match.matrix m.out$distance plot(m.out\\(distance, m.out\\)fitted.values) # same method: exact, subclass, optimal, full, cem distance: pscore plot(m.out, type = “qq”, interactive=FALSE) 5.3.4 Checking balance on covariates: 5.3.4.1 Balance for formula 1: plot(summary(m.out)) 5.3.4.2 Balance for formula 2: set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm2, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = TRUE, caliper = 0.2, discard = &#39;both&#39; ) plot(summary(m.out)) love.plot(m.out, binary = &quot;std&quot;) Standardized mean differences (SMD): summary(m.out) ## ## Call: ## matchit(formula = fm2, data = lalonde, method = &quot;nearest&quot;, distance = &quot;logit&quot;, ## discard = &quot;both&quot;, replace = TRUE, caliper = 0.2) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.6237 0.1623 1.8005 1.3810 0.4018 0.6783 ## age 25.8162 28.0303 -0.3094 0.4400 0.0813 0.1577 ## I(age^2) 717.3946 901.7786 -0.4276 0.3627 0.0813 0.1577 ## I(age^3) 21554.6595 32892.1142 -0.5408 0.2882 0.0813 0.1577 ## educ 10.3459 10.2354 0.0550 0.4959 0.0347 0.1114 ## black 0.8432 0.2028 1.7615 . 0.6404 0.6404 ## hispan 0.0595 0.1422 -0.3498 . 0.0827 0.0827 ## married 0.1892 0.5128 -0.8263 . 0.3236 0.3236 ## I(re74/1000) 2.0956 5.6192 -0.7211 0.5181 0.2248 0.4470 ## I(re75/1000) 1.5321 2.4665 -0.2903 0.9563 0.1342 0.2876 ## ## ## Summary of Balance for Matched Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.5961 0.5956 0.0022 0.9866 0.0027 0.0533 ## age 25.5089 25.9763 -0.0653 0.9175 0.0419 0.1361 ## I(age^2) 705.0237 733.0651 -0.0650 0.9945 0.0419 0.1361 ## I(age^3) 21242.4320 22390.9941 -0.0548 1.0300 0.0419 0.1361 ## educ 10.4438 10.1775 0.1324 0.3762 0.0495 0.1834 ## black 0.8284 0.8580 -0.0814 . 0.0296 0.0296 ## hispan 0.0651 0.0473 0.0751 . 0.0178 0.0178 ## married 0.2071 0.2426 -0.0906 . 0.0355 0.0355 ## I(re74/1000) 2.2766 2.2802 -0.0007 1.2066 0.0364 0.2722 ## I(re75/1000) 1.5084 1.3610 0.0458 1.7749 0.0272 0.1657 ## Std. Pair Dist. ## distance 0.0117 ## age 1.1503 ## I(age^2) 1.0985 ## I(age^3) 1.0232 ## educ 1.3037 ## black 0.1790 ## hispan 0.3253 ## married 0.6647 ## I(re74/1000) 0.7456 ## I(re75/1000) 0.6802 ## ## Sample Sizes: ## Control Treated ## All 429. 185 ## Matched (ESS) 47.21 169 ## Matched 78. 169 ## Unmatched 229. 0 ## Discarded 122. 16 #out = summary(m.out) #round(out$sum.all, 3) #round(out$sum.matched, 3) plot(m.out, type = &quot;hist&quot;, interactive = F) plot(m.out, type = &quot;jitter&quot;, interactive = F) love.plot(m.out, binary = &quot;std&quot;) bal.plot(m.out, var.name = &quot;distance&quot;, which = &quot;both&quot;, type = &quot;histogram&quot;, mirror = TRUE) who matched to whom? head(m.out$match.matrix, 10) ## [,1] ## NSW1 &quot;PSID15&quot; ## NSW2 &quot;PSID76&quot; ## NSW3 NA ## NSW4 &quot;PSID356&quot; ## NSW5 NA ## NSW6 &quot;PSID269&quot; ## NSW7 &quot;PSID269&quot; ## NSW8 &quot;PSID356&quot; ## NSW9 &quot;PSID253&quot; ## NSW10 &quot;PSID329&quot; 5.3.4.3 Balance for formula 3: set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm3, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = TRUE, caliper = 0.2, discard = &#39;both&#39; ) plot(summary(m.out)) love.plot(m.out, binary = &quot;std&quot;) 5.3.5 Estimation of treatment effect I will use the matching results with fm3 because it yielded the best balance on the covariates between the matched groups. I also kept ratio = 1 and replace = FALSE to avoid the specification of weights for now: set.seed(42) m.out &lt;- matchit(data = lalonde, formula = fm3, distance = &quot;logit&quot;, method = &quot;nearest&quot;, replace = FALSE, ratio = 1, caliper = 0.2, discard = &#39;both&#39; ) plot(summary(m.out)) love.plot(m.out, binary = &quot;std&quot;) Extract matched data: m.data &lt;- match.data(m.out) # m.data &lt;- get_matches(m.out) Linear model without covariates: fit1 &lt;- lm(re78 ~ treat, data = m.data) summ(fit1) ## MODEL INFO: ## Observations: 200 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(1,198) = 0.14, p = 0.71 ## R² = 0.00 ## Adj. R² = -0.00 ## ## Standard errors: OLS ## ----------------------------------------------------- ## Est. S.E. t val. p ## ----------------- --------- --------- -------- ------ ## (Intercept) 5555.31 744.75 7.46 0.00 ## treat 396.36 1053.23 0.38 0.71 ## ----------------------------------------------------- Cluster-robust standard errors: coeftest(fit1, vcov. = vcovCL) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5555.31 641.85 8.6552 1.705e-15 *** ## treat 396.36 1053.23 0.3763 0.7071 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Linear model with covariates: double robust (use with caution) fit2 &lt;- lm(re78 ~ treat + age + educ + black + hispan + married + nodegree + un74 + un75 + re74 + re75, data = m.data) summ(fit2) ## MODEL INFO: ## Observations: 200 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(11,188) = 2.78, p = 0.00 ## R² = 0.14 ## Adj. R² = 0.09 ## ## Standard errors: OLS ## ------------------------------------------------------ ## Est. S.E. t val. p ## ----------------- ---------- --------- -------- ------ ## (Intercept) 22.70 5049.51 0.00 1.00 ## treat -511.77 1076.16 -0.48 0.63 ## age 24.94 67.50 0.37 0.71 ## educ 524.42 323.09 1.62 0.11 ## black -1547.82 1329.82 -1.16 0.25 ## hispan 1274.60 2080.07 0.61 0.54 ## married -173.99 1327.78 -0.13 0.90 ## nodegree 978.45 1550.85 0.63 0.53 ## un74 -4299.86 1572.94 -2.73 0.01 ## un75 1927.21 1488.42 1.29 0.20 ## re74 0.28 0.14 1.99 0.05 ## re75 0.34 0.20 1.65 0.10 ## ------------------------------------------------------ coeftest(fit2, vcov. = vcovCL) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.69780 5654.73603 0.0040 0.99680 ## treat -511.76505 963.93187 -0.5309 0.59610 ## age 24.93687 68.98432 0.3615 0.71814 ## educ 524.42185 315.40948 1.6627 0.09804 . ## black -1547.81890 1155.69566 -1.3393 0.18209 ## hispan 1274.59576 1950.06532 0.6536 0.51416 ## married -173.99139 1146.14452 -0.1518 0.87950 ## nodegree 978.44625 1552.75781 0.6301 0.52937 ## un74 -4299.85657 1810.33847 -2.3752 0.01855 * ## un75 1927.21482 2178.34831 0.8847 0.37744 ## re74 0.27636 0.21661 1.2759 0.20358 ## re75 0.33714 0.24975 1.3499 0.17867 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.3.6 Hidden bias analysis extract matched treatment and control units from m.data psens(x = m.data$re78[m.data$treat==1], y = m.data$re78[m.data$treat==0], Gamma = 3, GammaInc=0.1) ## ## Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value ## ## Unconfounded estimate .... 0.5436 ## ## Gamma Lower bound Upper bound ## 1.0 0.5436 0.5436 ## 1.1 0.3837 0.6968 ## 1.2 0.2525 0.8124 ## 1.3 0.1566 0.8906 ## 1.4 0.0923 0.9393 ## 1.5 0.0521 0.9677 ## 1.6 0.0284 0.9834 ## 1.7 0.0150 0.9917 ## 1.8 0.0077 0.9960 ## 1.9 0.0039 0.9981 ## 2.0 0.0019 0.9991 ## 2.1 0.0009 0.9996 ## 2.2 0.0004 0.9998 ## 2.3 0.0002 0.9999 ## 2.4 0.0001 1.0000 ## 2.5 0.0000 1.0000 ## 2.6 0.0000 1.0000 ## 2.7 0.0000 1.0000 ## 2.8 0.0000 1.0000 ## 2.9 0.0000 1.0000 ## 3.0 0.0000 1.0000 ## ## Note: Gamma is Odds of Differential Assignment To ## Treatment Due to Unobserved Factors ## a gamma value with 1.6 or larger could lead to a change in the ATT estimate. 5.3.7 References: matchit: https://kosukeimai.github.io/MatchIt/articles/MatchIt.html manual: https://imai.fas.harvard.edu/research/files/matchit.pdf starting from page 15 Let’s play with the arguments! "],["propensity-score-analysis.html", "Chapter 6 Propensity Score Analysis 6.1 Data 6.2 Calculating Propensity Scores 6.3 IPTW: Inverse Probability Treatment Weighting 6.4 Stratification 6.5 Adjustment 6.6 Exercise", " Chapter 6 Propensity Score Analysis In this lab, we will use propensity scores to perform other types of analyses (weighting, stratification, and covariate adjustment) library(weights) library(survey) library(twang) library(CBPS) library(cobalt) library(jtools) library(lmtest) library(sandwich) #vcovCL library(rbounds) #gamma library(tidyr) library(tidyverse) library(janitor) 6.1 Data #data(lalonde) lalonde = MatchIt::lalonde dim(lalonde) ## [1] 614 9 names(lalonde) ## [1] &quot;treat&quot; &quot;age&quot; &quot;educ&quot; &quot;race&quot; &quot;married&quot; &quot;nodegree&quot; &quot;re74&quot; &quot;re75&quot; ## [9] &quot;re78&quot; 6.1.1 Create dummy variable for race and unemployment lalonde$black = ifelse(lalonde$race==&#39;black&#39;, 1, 0) lalonde$hispan = ifelse(lalonde$race==&#39;hispan&#39;, 1, 0) lalonde$un74 = ifelse(lalonde$re74==0, 0, 1) lalonde$un75 = ifelse(lalonde$re75==0, 0, 1) 6.2 Calculating Propensity Scores 6.2.1 Different formulas fm1 = treat ~ age + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm2 = treat ~ age + I(age^2) + I(age^3) + educ + black + hispan + married + I(re74/1000) + I(re75/1000) fm3 = treat ~ age + I(age^2) + I(age^3) + educ + I(educ^2) + black + hispan + married + I(re74/1000) + I(re75/1000) 6.2.2 Calculation of propensity scores (p-scores) pscore &lt;- glm(fm2, data = lalonde, family = &#39;binomial&#39;) head(pscore$fitted.values) ## NSW1 NSW2 NSW3 NSW4 NSW5 NSW6 ## 0.5871794 0.3014102 0.9056376 0.9020139 0.9069395 0.8230760 hist(pscore$fitted.values[lalonde$treat==0],xlim=c(0,1)) hist(pscore$fitted.values[lalonde$treat==1],xlim=c(0,1)) lalonde$pscore = pscore$fitted.values 6.3 IPTW: Inverse Probability Treatment Weighting In the following code, the weight variable weightATE is created by using the ifelse() function to obtain the inverse of the propensity score for treated units or the inverse of 1 minus the propensity score for control units lalonde$weightATE &lt;- with(lalonde, ifelse(treat==1, 1/pscore, 1/(1-pscore))) A summary of the ATE weights for treated and untreated groups is obtained with the following: with(lalonde, by(weightATE,treat,summary)) ## treat: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.026 1.067 1.441 1.233 10.579 ## ------------------------------------------------------------------------ ## treat: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.076 1.155 1.510 2.907 2.123 33.961 boxplot(lalonde$weightATE ~ lalonde$treat) the maximum weight for the ATE is 117.417 Individuals with extreme weights for the ATE are those who are either very likely to participate in the treatment given their covariate values but did not or very unlikely to participate but did so. perform weight truncation Truncation can be performed by assigning the weight at a cutoff percentile to observations with weights above the cutoff. The following code demonstrates weight truncation. More specifically, it uses the quantile function to calculate the weight at the 99th percentile and the ifelse function to assign this weight to any student whose weight exceeds the 99th percentile: lalonde$weightATETruncated &lt;- with(lalonde, ifelse(weightATE &gt; quantile(weightATE, 0.99), quantile(weightATE, 0.99), weightATE)) may truncate all above .05 quantiles if necessary with(lalonde, by(weightATETruncated,treat,summary)) ## treat: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.026 1.067 1.441 1.233 10.579 ## ------------------------------------------------------------------------ ## treat: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.076 1.155 1.510 2.530 2.123 12.546 boxplot(lalonde$weightATETruncated ~ lalonde$treat) 6.3.1 Balance check In this section, covariate balance evaluation is performed by comparing the standardized difference between the weighted means of the treated and untreated groups. The bal.stat function of the twang package (Ridgeway et al., 2013) is useful for covariate balance evaluation. If there are no sampling weights, then sampw=1. covariateNames &lt;-names(lalonde)[3:14] balanceTable &lt;- bal.stat(lalonde, vars= covariateNames, treat.var = &quot;treat&quot;, w.all = lalonde$weightATETruncated, get.ks=F, sampw = 1, estimand=&quot;ATE&quot;, multinom=F) balanceTable &lt;- balanceTable$results round(balanceTable,3) ## tx.mn tx.sd ct.mn ct.sd std.eff.sz stat p ## educ 10.460 2.012 10.258 3.002 0.077 0.654 0.513 ## race:black 0.531 0.499 0.401 0.490 0.267 3.275 0.039 ## race:hispan 0.163 0.370 0.117 0.322 0.144 NA NA ## race:white 0.305 0.461 0.482 0.500 -0.354 NA NA ## married 0.292 0.456 0.408 0.492 -0.236 -1.857 0.064 ## nodegree 0.659 0.475 0.573 0.495 0.178 1.374 0.170 ## re74 3258.732 6878.773 4513.602 6295.593 -0.194 -1.226 0.221 ## re75 1997.046 3474.214 2137.977 3144.448 -0.043 -0.327 0.744 ## re78 7169.346 7942.905 6349.181 6943.242 0.110 0.792 0.429 ## black 0.531 0.500 0.401 0.491 0.267 1.957 0.051 ## hispan 0.163 0.371 0.117 0.322 0.144 0.862 0.389 ## un74 0.386 0.488 0.677 0.468 -0.594 -4.450 0.000 ## un75 0.498 0.501 0.623 0.485 -0.255 -1.953 0.051 ## pscore 0.390 0.308 0.306 0.316 0.268 1.938 0.053 std.eff.sz quantifies effect size 0.2 small 0.5 medium 0.8 large most of them are medium to large - balance was not achieved * 6.3.2 Estimation of treatment effect the final weights are divided by the mean of weights to make them sum to the sample size, which is a process known as normalization. lalonde$finalWeight &lt;- lalonde$weightATETruncated/mean(lalonde$weightATETruncated) Before the estimation can be performed, the surveyDesign object is created with the svydesign function to declare the names of the variables that contain cluster ids, strata ids, weights, and the data set: surveyDesign &lt;- svydesign(ids=~1, weights=~finalWeight, data = lalonde, nest=T) Methods to obtain standard errors for propensity score weighted estimates include Taylor series linearization and resampling methods such as bootstrapping, jackknife, and balanced repeated replication (see review by Rodgers, 1999). To use bootstrap methods with the survey package, the surveyDesign object created above should be modified to include weights for each replication. The following code takes the surveyDesign object and adds weights for 1,000 bootstrapped samples: set.seed(8) surveyDesignBoot &lt;- as.svrepdesign(surveyDesign, type=c(&quot;bootstrap&quot;), replicates=1000) First, the svyby function is used to apply the svymean function separately to treated and control units to obtain weighted outcome: weightedMeans &lt;- svyby(formula=~re78, by=~treat, design=surveyDesignBoot, FUN=svymean, covmat=TRUE) weightedMeans ## treat re78 se ## 0 0 6349.181 394.5367 ## 1 1 7169.346 956.6697 The code to obtain the treatment effect with a regression model is shown as follows. The model formula using R notation is re78~treat. This formula can be expanded to include any covariates and interaction effects of interest outcomeModel &lt;- svyglm(re78~treat,surveyDesignBoot) summary(outcomeModel) # final model ## ## Call: ## svyglm(formula = re78 ~ treat, surveyDesignBoot) ## ## Survey design: ## as.svrepdesign.default(surveyDesign, type = c(&quot;bootstrap&quot;), replicates = 1000) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6349.2 394.5 16.09 &lt;2e-16 *** ## treat 820.2 1025.7 0.80 0.424 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 33407668806) ## ## Number of Fisher Scoring iterations: 2 6.4 Stratification The following code used the cut function to create five strata of approximately the same size based on the quintiles of the distribution of propensity scores for both treated and untreated groups. The quintiles are obtained with the quantile function. The function levels is used to assign number labels from 1 to 5 to the strata, and then xtabs is used to display strata by treatment counts. hist(lalonde$pscore) quantile(lalonde$pscore, prob = seq(0, 1, 1/5)) ## 0% 20% 40% 60% 80% 100% ## 0.0004734592 0.0298866776 0.0730729257 0.2744818790 0.6466930310 0.9295361003 lalonde$subclass &lt;- cut(x=lalonde$pscore, breaks = quantile(lalonde$pscore, prob = seq(0, 1, 1/5)), include.lowest=T) levels(lalonde$subclass) &lt;- 1:length(levels(lalonde$subclass)) ntable &lt;- xtabs(~treat+subclass,lalonde) ntable ## subclass ## treat 1 2 3 4 5 ## 0 122 119 103 61 24 ## 1 1 4 19 62 99 surveyDesign &lt;- svydesign(ids=~1, weights=~finalWeight, data = lalonde, nest=T) To use bootstrap methods with the survey package, the surveyDesign object created above should be modified to include weights for each replication. The following code takes the surveyDesign object and adds weights for 1,000 bootstrapped samples: set.seed(8) surveyDesignBoot &lt;- as.svrepdesign(surveyDesign, type=c(&quot;bootstrap&quot;), replicates=1000) The following R code uses svyby to apply the svymean function: head(lalonde) ## treat age educ race married nodegree re74 re75 re78 black hispan un74 un75 ## NSW1 1 37 11 black 1 1 0 0 9930.0460 1 0 0 0 ## NSW2 1 22 9 hispan 0 1 0 0 3595.8940 0 1 0 0 ## NSW3 1 30 12 black 0 0 0 0 24909.4500 1 0 0 0 ## NSW4 1 27 11 black 0 1 0 0 7506.1460 1 0 0 0 ## NSW5 1 33 8 black 0 1 0 0 289.7899 1 0 0 0 ## NSW6 1 22 9 black 0 1 0 0 4056.4940 1 0 0 0 ## pscore weightATE weightATETruncated finalWeight subclass ## NSW1 0.5871794 1.703057 1.703057 0.9625366 4 ## NSW2 0.3014102 3.317738 3.317738 1.8751250 4 ## NSW3 0.9056376 1.104194 1.104194 0.6240705 5 ## NSW4 0.9020139 1.108630 1.108630 0.6265776 5 ## NSW5 0.9069395 1.102609 1.102609 0.6231747 5 ## NSW6 0.8230760 1.214955 1.214955 0.6866701 5 subclassMeans &lt;- svyby(formula=~re78, by=~treat+subclass, design=surveyDesignBoot, FUN=svymean, covmat=TRUE) ## Warning in svrVar(repmeans, scale, rscales, mse = design$mse, coef = rval): 350 replicates gave ## NA results and were discarded. ## Warning in svrVar(repmeans, scale, rscales, mse = design$mse, coef = rval): 21 replicates gave NA ## results and were discarded. ## Warning in svrVar(replicates, design$scale, design$rscales, mse = design$mse, : 360 replicates ## gave NA results and were discarded. subclassMeans ## treat subclass re78 se ## 0.1 0 1 8774.975 7.401116e+02 ## 1.1 1 1 6788.463 4.072737e-14 ## 0.2 0 2 7333.718 7.000384e+02 ## 1.2 1 2 3330.084 1.245754e+03 ## 0.3 0 3 6213.953 6.240724e+02 ## 1.3 1 3 10249.964 2.443014e+03 ## 0.4 0 4 4601.847 7.941464e+02 ## 1.4 1 4 5616.417 6.939908e+02 ## 0.5 0 5 4819.586 1.158452e+03 ## 1.5 1 5 6501.883 8.881021e+02 To obtain the ATE or ATT by pooling stratum-specific effects, svycontrast is used with the weights First, use ntable to obtain the weights ntable ## subclass ## treat 1 2 3 4 5 ## 0 122 119 103 61 24 ## 1 1 4 19 62 99 colSums(ntable)/sum(ntable) ## 1 2 3 4 5 ## 0.2003257 0.2003257 0.1986971 0.2003257 0.2003257 ATTw = colSums(ntable)/sum(ntable) This won’t work: pooledEffects &lt;- svycontrast(subclassMeans, contrasts = list(ATT=ATTw)) ATTw needs to be specified for EVERY group within EVERY stratum in our case, 9 groups (no treatment in first stratum so 2*5-1 = 9) Control groups get negative weights and treatment group gets positive weights: ATTw2 &lt;- c(-ATTw[1], ATTw[1], # first stratum, just control -ATTw[2], ATTw[2], # second stratum -ATTw[3], ATTw[3], # third stratum -ATTw[4], ATTw[4], # fourth stratum -ATTw[4], ATTw[5]) # fifth stratum subclassMeans$ATTw2 &lt;- ATTw2 view ATTw2 here: subclassMeans ## treat subclass re78 se ATTw2 ## 0.1 0 1 8774.975 7.401116e+02 -0.2003257 ## 1.1 1 1 6788.463 4.072737e-14 0.2003257 ## 0.2 0 2 7333.718 7.000384e+02 -0.2003257 ## 1.2 1 2 3330.084 1.245754e+03 0.2003257 ## 0.3 0 3 6213.953 6.240724e+02 -0.1986971 ## 1.3 1 3 10249.964 2.443014e+03 0.1986971 ## 0.4 0 4 4601.847 7.941464e+02 -0.2003257 ## 1.4 1 4 5616.417 6.939908e+02 0.2003257 ## 0.5 0 5 4819.586 1.158452e+03 -0.2003257 ## 1.5 1 5 6501.883 8.881021e+02 0.2003257 pooledEffects &lt;- svycontrast(subclassMeans, list(ATT=as.numeric(ATTw2))) pooledEffects ## contrast SE ## ATT 142.22 557.7 6.5 Adjustment adjustModel &lt;- lm(re78~treat+pscore,lalonde) summ(adjustModel) ## MODEL INFO: ## Observations: 614 ## Dependent Variable: re78 ## Type: OLS linear regression ## ## MODEL FIT: ## F(2,611) = 4.29, p = 0.01 ## R² = 0.01 ## Adj. R² = 0.01 ## ## Standard errors: OLS ## ------------------------------------------------------ ## Est. S.E. t val. p ## ----------------- ---------- --------- -------- ------ ## (Intercept) 7569.64 416.59 18.17 0.00 ## treat 1029.49 888.59 1.16 0.25 ## pscore -3607.64 1304.77 -2.76 0.01 ## ------------------------------------------------------ 6.6 Exercise Exercise: can you use fm2 to obtain propensity scores and see if that improves the balance and thus the estimate of treatment effect? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
